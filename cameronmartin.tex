\documentclass{scrartcl}
\usepackage[latin9]{inputenc}  
\usepackage[T1]{fontenc}
\usepackage{kantlipsum}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage{amsmath,amsthm,amssymb,upref}
\usepackage[left=1cm, right = 1cm, top = 2cm, bottom = 3cm]{geometry}
\usepackage{todonotes}
%\usepackage[usenames,dvipsnames]{xcolor}
\usepackage[breakable, theorems, skins]{tcolorbox}
\usepackage[framemethod=TikZ]{mdframed}
\definecolor{bluebg}{rgb}{0.8,0.8,0.97}
\definecolor{graybg}{rgb}{0.9,0.9,0.9}
\tcbset{enhanced}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Proposition}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newmdtheoremenv [backgroundcolor=bluebg, %
innertopmargin =0pt , %
splittopskip = \topskip, % 
skipbelow= 6pt, %
skipabove=6pt, %
topline=false,bottomline=false,leftline=false,rightline=false, roundcorner=10pt]{example}{Example}
\newmdtheoremenv [backgroundcolor=graybg, %
innertopmargin =0pt , %
splittopskip = \topskip, % 
skipbelow= 6pt, %
skipabove=6pt, %
topline=false,bottomline=false,leftline=false,rightline=false, roundcorner=10pt]{background}{Background Info}


\newcommand{\de}{\mathrm d}
\newcommand{\Frechet}{Fr\'echet }
\newcommand{\T}{\mathcal T}
\newcommand{\eps}{\varepsilon}
\newcommand{\N}{\mathbb N}
\newcommand{\R}{\mathbb R}
\newcommand{\Q}{\mathbb Q}
\newcommand{\E}{\mathbb E}
\newcommand{\ednote}[1]{{\color{red}[#1]}}
\DeclareMathSymbol{\mlq}{\mathord}{operators}{``}
\DeclareMathSymbol{\mrq}{\mathord}{operators}{`'}
\newcommand{\del}{\partial}
\newcommand{\dist}{\operatorname{dist}}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=2pt] (char) {#1};}}
\DeclareRobustCommand{\mybox}[2][gray!20]{%
\begin{tcolorbox}[   %% Adjust the following parameters at will.
        breakable,
        left=0pt,
        right=0pt,
        top=0pt,
        bottom=0pt,
        colback=#1,
        colframe=#1,
        width=\dimexpr\textwidth\relax, 
        enlarge left by=0mm,
        boxsep=5pt,
        arc=0pt,outer arc=0pt,
        ]
        #2
\end{tcolorbox}
}




\DeclareRobustCommand{\myboxtwo}[2][blue!20]{%
\begin{tcolorbox}[   %% Adjust the following parameters at will.
        breakable,
        left=0pt,
        right=0pt,
        top=0pt,
        bottom=0pt,
        colback=#1,
        colframe=#1,
        width=\dimexpr\textwidth\relax, 
        enlarge left by=0mm,
        boxsep=5pt,
        arc=10pt,outer arc=10pt,
        ]
        #2
\end{tcolorbox}
}


\title{The Hitchhiker's Guide to Cameron-Martin theory}
\author{Philipp Wacker, FAU Erlangen-N\"urnberg}
\date{\today, Version 0.9}
\usepackage{hyperref}
\begin{document}
\maketitle

\section{Introduction}
I assume you, the reader, want to learn about Cameron-Martin spaces and what they are all about. There are extraordinary books (e.g. \cite{bogachev1998gaussian, da2014stochastic}) about this (and I assume you have looked at a few of them) but they tend to be quite terse and not exceedingly illustrative. This article is supposed to help you build intuition for notions like \textit{reconstructing kernel Hilbert spaces}, the \textit{Cameron-Martin space} of a Gaussian measures in Banach spaces and shifts of Gaussians. 

This exposition relies heavily on two sources: The lecture notes of the 19th Internet seminar on Infinite dimensional analysis, \cite{Lunardi}, and Nate Eldredge's wonderful lecture notes on analysis and probability in infinite dimensions \cite{eldredge2016analysis}. I have stolen shamelessly from both notes, trying to combine the ideas from both treatises which helped me the most when I was trying to understand Cameron-Martin spaces. This document deals only with a small fraction of both sources, i.e. the first few introductory chapters about infinite-dimensional probability measures.

\subsection{How to read this document}
Normal text usually explains and introduces necessary notation in general Banach spaces, 
\mybox{gray boxes explain necessary background math  in more detail and}
\myboxtwo{blue boxes contain worked examples in both finite and infinite settings. Examples are numbered by \circled{1}, \circled{2} etc. and are continued throughout the text, i.e. example \circled{3} pops up multiple times in the text, applying the current important ideas to the same concrete example.}
\subsection{Where do I find ...?}
\begin{itemize}
\item Gaussian measures in Banach spaces: section \ref{sec:GaussBanach}
\item Reconstructing kernel Hilbert spaces: section \ref{sec:rkhs}
\item Cameron-Martin spaces: section \ref{sec:CM}
\item Gaussian measures in Hilbert spaces: Example \circled{4} in section \ref{sec:InfDimEx}
\item The classical Wiener space: section \ref{sec:InfDimEx}
\end{itemize}
\subsection{Who is responsible for this mess?}
Credit for all good parts is due to Nate Eldredge, Alessandra Lunardi, Michele Miranda and Diego Pallara, whose excellent lecture notes have helped me grasp everything I now understand about Gaussian measures in infinite-dimensional spaces. All bad explanations, mathematical errors and other deficiencies are solely my fault. I am happy to receive comments or questions about these notes and you can reach me via \texttt{wacker@math.fau.de}. These notes are still under construction so check for newer versions.
\section{Gaussian measures in $\R^d$}
A Gaussian measure $\gamma$ on $\R^d$ is a probability measure with characteristic function (Fourier transform)
\[ \hat \gamma(f) := \int_{\R^d} e^{if^Tx}\gamma(\de x) = \exp\left\{i\cdot f^Ta - \frac{1}{2}f^T\Sigma f\right\} \]
for some $m\in\R^d$ (we call this the mean) and $\Sigma \in\R^{d\times d}$ symmetric and positively semi-definite (we call this the covariance matrix). In short, $\gamma = N(m, \Sigma)$. 
We can also obtain mean and covariance by integration:
\[ m = \int_{\R^d} x \, \gamma(\de x),\quad \text{or } f^Tm = \int_{\R^d} f^T x \, \gamma(\de x) \text{ for } f\in\R^d \]
and 
\[ \Sigma = \int_{\R^d} (x-m)\cdot (x-m)^T \, \gamma(\de x), \quad \text{or } f^T\Sigma g = \int_{\R^d}f^T(x-m) \cdot g^T(x-m)\, \gamma(\de x). \]

The latter definitions in each of the last two expressions allow indirect calculation of $m$ and $\Sigma$ by ``testing'' with suitable $f,g\in \R^d$. The best method for that is choosing $f=(1,0,\ldots, 0)=e_1$, then $f = (0, 1, 0, \ldots, 0)=e_2$ etc. when calculating $f^Tm$ and all combinations for $f$ and $g$ for the calculation of $f^T\Sigma g$ and then $m = (e_1^Tm, e_2^Tm,\ldots, e_d^T m)$ and $\Sigma = (\Sigma_{i,j})_{i,j=1}^d$ with $\Sigma_{i,j} = e_i^T\Sigma e_j$. 

It may look strange to calculate mean and variance in that way if there is an explicit formula for $m$ and $\Sigma$ themselves, but the ``testing method'' generalizes more easily to infinite dimensions without the use of Bochner integrals (note that the expressions for $m$ and $\Sigma$ are actually vector-valued and matrix-valued)!

In finite dimensions we have an expression for the density of the measure with respect to the Lebesgue measure on $\R^d$ if $\Sigma$ is non-singular:
\[ \frac{\de \gamma}{\de\lambda}(x) = \frac{1}{\sqrt{(2\pi)^d \det \Sigma}}\exp\left\{- \frac{x^T\Sigma^{-1}x}{2}\right\}. \]
This is nice for calculations in finite dimensions but completely worthless in the context of infinite dimensions as there is no analogue of Lebesgue measure there. So better forget this immediately and focus on the characterization via its Fourier transform. Another definition that generalizes easily to the infinite-dimensional case is via projection on one component: Given any $f\in\R^d$, a vector $x\in\R^d$ is distributed according to a Gaussian measure if and only if $f^T x$ is a one-dimensional Gaussian measure with mean $f^Tm$ and variance $f^T\Sigma f$.
\section{Gaussian measures in vector spaces}\label{sec:GaussBanach}
Let $X$ be a topological vector space (think of a Banach space -- or a \Frechet space if you're feeling adventurous). A Gaussian measure is a probability measure on $X$ if and only if for each continuous linear functional $f\in X^*$ the push-forward measure $\gamma \circ f^{-1}$ is a Gaussian measure on $\R$.

We can define a mean and a covariance by setting
\begin{itemize}
\item $a_\gamma(f) = \int_X f(x)\gamma(\de x)$, so $a_\gamma: X^*\to \R$, and hence $a_\gamma \in (X^*)'$.
\item $B_\gamma(f, g) = \int_X [f(x) - a_\gamma(f)]\cdot [g(x)-a_\gamma(g)]\gamma(\de x)$, so $B_\gamma \in (X^*\times X^*)'$.
\end{itemize}
So far we only know that $a_\gamma$ is a linear map from $X^*$ (the so-called topological dual) to $\R$, i.e. $a_\gamma$ is an element of the ``algebraical dual of the topological dual'', because we don't know yet whether $a_\gamma$ is continuous (i.e. bounded). The exponential Gaussian moments we obtain from Fernique's theorem prove exactly this, and thus actually $a_\gamma \in X^{**}$. This is still kind of impractical: We can interpret $a_\gamma(f)$ as the mean of the measure ``in direction of $f$'', so we would like to view $a_\gamma$ as the ``general mean'' of the measure. But we think of the mean as an object in the probability space (think of sampling from a measure; the empirical mean will lie in the same space $X$ as the samples), but here we can only state $a_\gamma \in X^{**}$. Because $X \subsetneq X^{**}$, we need to do some work in order to identify $a_\gamma$ with an actual element in $X$. 

\mybox[gray!20]{
\subsection*{What's the deal with $X\subsetneq X^{**}$?}
Every $x\in X$ can also be interpreted as an object in $X^{**}$ in the following sense: Recall that $\phi \in X^{**} = (X^*)^*$ takes an element of the dual space $f\in X^*$ and maps it linearly and continuously into $\R$. So how can we -- given just an element $x\in X$ which is no map whatsoever -- map $f\in X^*$ continuously into $\R$? The following works: For $x\in X$ define $\phi_x \in X^{**}$ by $\phi_x(f) = f(x)$ with $f\in X^*$. 
\begin{itemize}
\item This is linear in $f$: Take $f,g\in X^*$ and $\alpha\in \R$, then $\phi_x(\alpha f+g) = (\alpha f+g)(x) = \alpha f(x) + g(x) = \alpha \phi_x(f) + \phi_x(g)$.
\item This is a continuous (i.e. bounded) mapping: $|\phi_x(f)| = |f(x)| \leq \|f\|_{X^*} \cdot \|x\|_X$ and thus $\|\phi\|_{X^{**}} \leq \|x\|_X$. We can actually show $\|\phi\|_{X^{**}} = \|x\|_X$, i.e. the mapping $J: X\to J(X)\subset X^{**}, x\mapsto \phi_x$ is an isometry! 
\end{itemize} 
In general, $J$ is not surjective, i.e. $X \subsetneq X^{**}$. By common laziness, we like to identify $x\in X$ with $J(x) = \phi_x\in X^{**}$ and just use the notation $x \in X^{**}$. This leads to confusing looking terms like $f(x) = x(f)$ for $f\in X^*$ (especially later when we talk about the map $R_\gamma$). We need to keep in mind how this is to interpreted.
\subsection*{When is an element $\phi\in X^{**}$ also in $X$?}
We established that $X$ is a proper subset of $X^{**}$, so we can't expect to be able to find an element $x_\phi\in X$ for any $\phi \in X^{**}$ such that $\phi(f) = f(x_\phi)$ for any $f\in X^*$ (note that we have reversed the notation as we are looking for $x$ given $\phi$). There is a powerful criterion when this is actually true:
\begin{prop}\label{prop:weakstar}
Let $\phi : X^*\to \R$ be a linear functional, i.e. $\phi\in (X^*)'$. If $\phi$ is continuous for the weak* topology, there exists a $x_\phi\in X$ such that for all $f\in X^*$
\[\phi(f) = f(x_\phi).\]
\end{prop}
\begin{itemize}
\item Note that we start with an element $\phi \in (X^*)'$ instead of in $(X^*)^*$, i.e. we only claim linearity.
\item Recall that in a separable Banach space it is enough to show sequential weak* continuity, i.e. if $\lim_{n\to \infty}\phi(f_n) = \phi(f)$ for every sequence $f_n \stackrel {*}{\rightharpoonup} f$, i.e. $\lim_{n\to\infty} f_n(x) = f(x)$ for every $x\in X$.
\item Don't forget: weak* continuity is a stronger condition than (regular) continuity! So it is not enough to show that $\phi\in X^{**}$ to obtain an element $x_\phi$ as needed. This is obvious from the fact that this would show $X^{**} \subset X$ which is false in general (for non-reflexive spaces). Another way to see this is that weak* continuity means that $\phi$ is continuous with respect to all weakly converging sequences. As weak* convergence is a weaker criterion than strong convergence (don't mix this up with weak* continuity), there are more sequences that are weakly* convergent that strongly convergent and thus the functional $\phi$ has to behave even better to be continuous with this bigger set of critical sequences.
\end{itemize}
The main point to take away from this proposition is that any linear functional $\phi : X^{*}\to \R$ on a separable Banach space $X$ has a corresponding $x_\phi\in X$ if it is sequentially weak*-continuous, which is just a fancy way of saying that for any pointwise converging sequence $(f_n)\subset X^*$, i.e. $f_n(x) \to f(x)$, we also have $\phi(f_n) \to \phi(f)$. \textbf{In all our applications here, $\phi$ will be an integral operator and we can show this criterion by integral convergence theorems like the dominated convergence theorem.}  
}
A main theorem when working with Gaussian measures on Banach spaces is Fernique's theorem which provides bounds for all polynomial moments. The proof is not too difficult but quite technical and so we will not write it here.
\begin{theorem}\label{thm:fernique}
Let $\gamma$ be a Gaussian measure on a separable Banach space $X$. Then there is an $\alpha > 0$ such that
\[\int_X \exp\left\{\alpha \|x\|^2\right\}\,\gamma(\de x) < \infty. \]
\end{theorem}
Now we're ready to prove that\footnote{or rather that for a given $\gamma$, there is an element $a\in X$ such that $a_\gamma\in X^{**}$ and $a\in X$ correspond in the sense $a_\gamma(f) = f(a)$.} $a_\gamma \in X$.
\begin{lemma}\label{lem:agamma}
Let $X$ be a separable Banach space and $\gamma$ a Gaussian measure on $X$. Then $a_\gamma \in X$.
\end{lemma}
\begin{proof}
$a_\gamma$ is an element of $(X^*)'$ (and actually $X^{**}$, but we won't need this explicitly). From proposition \ref{prop:weakstar} we know that we only need to prove that $a_\gamma$ is weak-*-continuous. Separability of $X$ means that sequential weak-*-continuity is actually enough. Hence we need to prove that for any $f_n \stackrel{*}{\rightharpoonup} f$, i.e. $f_n(x)\to f(x)$ for any $x\in X$, 
\[ a_\gamma(f_n) \to a_\gamma(f). \]
As $X\subsetneq X^{**}$, this is not true in general, but the special structure of $a_\gamma$ as a Lebesgue integral functional (and hence we can use the Lebesgue dominated convergence theorem) and the good integrability properties of Gaussian measures -- courtesy of Fernique's theorem -- help us out. First we realize that any weak-*-convergent sequence is bounded, hence $\sup_n\|f_n\|_{X^*}  \leq C$. Then pointwise convergence of $f_n$ and 
\[ |a_\gamma(f_n)| \leq \int_X \|f_n\|_{X^*} \|x\|\,\gamma(\de x) \leq C \int_X \|x\|\,\gamma( \de x) < C' \]
where the last bound follows from Theorem \ref{thm:fernique}. Then Lebesgue's dominated convergence theorem immediately yields the result.
\end{proof}
\myboxtwo{\subsection*{Example \circled{1}: Mean and Covariance in finite dimensions}
Take a multivariate Gaussian in $d$ dimensions, i.e. $\gamma = N(m, \Sigma)$ with $m\in\R^d$ and $\Sigma\in\R^{d\times d}$. Then linear functionals $f\in X^*$ can be interpreted as $\R^d$ vectors, with $f(x) = f^Tx$. Then 
\[a_\gamma(f) = \int_{\R^d}f^Tx \, \gamma(\de x) = f^T m.\] By the duality voodoo explained above, although $a_\gamma \in X^{**}$ (technically), we can say $a_\gamma\in X$ by identifying $a_\gamma(f) = f(a_\gamma) = f^T a_\gamma$, so actually $a_\gamma = m$ as expected and $a_\gamma(f) = f^Tm$. One has to be careful not to mix up $a_\gamma$ (the mean of the Gaussian) and $a_\gamma(f)$ (the mean of the Gaussian in direction $f$). This is even more true in the general Banach space setting.

The covariance form is 
\[B_\gamma(f, g) = \int_{\R^d}[f(x)-a_\gamma(f)]\cdot [g(x)-a_\gamma(g)]\, \gamma(\de x) = \int_{R^d}f^T(x-m)\cdot g^T(x-m) \, \gamma(\de x) = f^T\Sigma g\]
}

\myboxtwo{\subsection*{Example \circled{2}: Mean and Covariance for an infinite product of standard Gaussians}
Take the Gaussian $\gamma = \bigotimes_{j=1}^\infty N(0,1)$, i.e. a measure on the sequence space $\R^\infty = \{(a_j)_{j\in\N}: a_j\in \R\}$. This is no Banach space but a \Frechet space (visit the appendix for this). A continuous linear functional is of the form $f\in X^* = c_{00}$, i.e. $f$ can be written as a real sequence that is eventually zero: $f = (f_1, f_2, \ldots, f_N, 0, 0, \ldots)$. Then, $f(x) = \sum_{j=1}^N f_jx_j$ where $N$ depends on the specific $f$ used. An arbitrary sequence (being not eventually zero) is not a continuous linear functional! This statement is proven in lemma \ref{lem:Xstar}.

\begin{lemma}\label{lem:Xstar}
Every continuous linear functional $f\in X^*$ is of the form 
\begin{equation}f(x) = \sum_{i=1}^n a_i x_i \label{eq:darstellung_f} \end{equation}
for some $a_1,\ldots, a_n\in\R$ and with the notation $X \ni x = (x_1, x_2,\ldots)$. Thus $X^*$ can be identified with $c_{00}$, the set of all real sequences which are eventually zero.
\end{lemma}
\begin{proof}
Let $f\in X^*$. Define $a_i = f(e_i)$. We show that only finitely many $a_i$ are nonzero. Indeed, assume that for any $N\in \N$ there is an $i_N > N$ such that $a_{i_N} \neq 0$.

As we assumed $f$ to be continuous, for any $\eps > 0$ there is an open neighborhood $U$ of $0$ such that

 \begin{equation}
\sup_{u\in U} |f(u)| < \eps.  \label{lem:cont}
 \end{equation} 
 By the form of the topology (cf. lemma \ref{lem:RInfTop}), this $U$ is of the form
\[ U =  \bigotimes_{i=1}^n U_i \times \R^\infty \]
for some $n\in \N$. Now we can define a sequence $(u_N)_N$ of elements $u_N$ (which are sequences) in $X$ as $u_N = \frac{1}{a_{i_N}}\cdot N\cdot e_{i_N }$, i.e. the $N$-th element is the sequence which has the value $\frac{N}{a_{i_N}}$ at the $i_N$-th position and $0$ elsewhere. For $M$ high enough, $(u_N)_{N\geq M}$ is a sequence in $U$. But
\[ f(u_N) = \frac{N}{a_{i_N}}\cdot f(e_{i_N}) = N \xrightarrow{N\to\infty} \infty,\]
hence \eqref{lem:cont} is violated and $f$ cannot be continuous.

This means that $f$ acts on only a finite number of basis elements of $X$ and as $f$ is linear we can write it in the form \eqref{eq:darstellung_f}.
\end{proof}

 Mean and covariance form (set $f,g\in X^*=c_{00}$) are
\[a_\gamma(f) = 0\in\R^\infty\]
and 
\[B_\gamma(f, g) = \sum_{j\in\N} f_j g_j \]
 where the sum is actually finite.
 
 

So the projections of elements $x\in X$ on the coordinates are i.i.d. Gaussians.
\begin{lemma}\label{lem:Gaussian}
$\mu$ is a Gaussian measure. The covariance form of $\mu$ is given by
\[ B_\gamma(f,g) = \sum_{i=1}^\infty f(e_i)g(e_i) \]
where the sum is actually finite.
\end{lemma}
\begin{proof}
We argue by using characteristic functions. 
\begin{align*}
\int_X e^{if(x)}\mu(dx) &= \int_{\R^\infty} e^{i\sum_{j=1}^n f(e_j) x_j}\, \bigotimes_{j=1}^\infty N(0,1)(dx_j) = \prod_{j=1}^n \int_\R e^{if(e_j)x_j}N(0,1)(dx_j)\\
&= \prod_{j=1}^n e^{-\frac{f(e_j)^2}{2}} = e^{-\frac{1}{2}\sum_{j=1}^n f(e_j)^2}
\end{align*}
and thus the covariance form is $B_\gamma(f, f) = \sum_{j=1}^n f(e_j)^2$ and hence $B_\gamma(f,g)=\sum_{j=1}^\infty f(e_j)g(e_j)$ where the sum runs until the largest non-zero entry of both $f$ and $g$.
\end{proof}
\begin{lemma}
$\mu$ has full support, i.e. there is no open set other than the empty set having zero measure.
\end{lemma}
\begin{proof}
Let $U$ be open in $X$, i.e. $U = \bigotimes_{i=1}^n U_i \times \R^\infty$ with $U_j$ open in $\R$. Then $\mu(U) = \prod_{j=1}^n N(0,1)(U_j) > 0$ as $N(0,1)$ has full support on $\R$.
\end{proof}
\begin{remark}
$B_\gamma$ is actually positive definite: The only $f\in X^*$ with $B_\gamma(f, f) = 0$ is $f = 0$. This means that $i: X^*\hookrightarrow L^2(X, \mu)$ with inner product $B_\gamma$ is an injection in the sense that $i(X^*) \subset L^2$ is isomorphic to $X^*$. This is actually cool because normally there may be $0 \neq f \in X^*$ with $0 = f \in L^2$ (for example in spaces with degenerate Gaussians like $X= \R^2$ with $N(0,1) \otimes \delta_0$. Here, choosing $f$ as the projection on the second coordinate (written as $f = (0,1)^T$) is not $0$ as a linear functional in $X^* = \R^2$ but $f(x) = 0$ $\mu$-almost surely and thus $0 = f\in L^2$ as a square-integrable random variable.
So, to reiterate, $X^*\hookrightarrow L^2(X, \mu)$ is an injection and we can view $X^*$ as a \textbf{subspace} of $L^2$. This is because we don't lose ``information'' by viewing $f$ as an element in $L^2$. In the example above with $\R^2$ we do lose information because after identifying $X^*$ with a subset of $L^2$, we can't distinguish $(0,1)^T$ and $(0,0)^T$ as random variables in $L^2$, although they are quite distinct in $X^* = \R^2$.
\end{remark}

\begin{remark}
As was pointed out, we can think of $X^*$ as a subset of $L^2(X,\mu)$ with inner product $q$. But: $X^*$ is not complete in the $B_\gamma$ inner product. This is the topic of the next section where we define $X_\gamma^* = \overline{X^*}^{L^2(X,\mu)}$, the $L^2$ -closure of $X^*$.
\end{remark}
 
}

\myboxtwo{
\subsection*{Example \circled{3}: Mean and Covariance for a degenerate Gaussian in 2-d}
An important example is $\gamma = \delta_p \otimes N(q, \sigma^2)$, the product of a one-dimensional Dirac measure (or a degenerate one-dimensional Gaussian) centred at $p\in\R$ and a one-dimensional Gaussian centred at $q\in\R$ with variance $\sigma^2$. This is a (degenerate) Gaussian measure on $X=\R^2$. Linear continuous functionals $f\in X^*$ can be written as vectors in $\R^2$ via the usual correspondence $f(x) = f^Tx$, i.e. $X^* = \R^2$. The mean and covariance are computed as follows:
\begin{align*}
a_\gamma(f) = \int_\R\int_\R (f_1x_1+f_2x_2) \delta_p(\de x_1) N(q, \sigma^2)(\de x_2) = f_1\cdot p + f_2\cdot q = f^T \begin{pmatrix}
p\\q
\end{pmatrix}
\end{align*}
for $f\in X^*=\R^2$ or more succinctly $a_\gamma = (p,q)^T$. With another $g\in X^*=\R^2$
\begin{align*}
B_\gamma(f,g) &= \int_\R\int_\R  (f_1,f_2)\begin{pmatrix}
x_1-p\\x_2-q
\end{pmatrix}\cdot (g_1,g_2)\begin{pmatrix}
x_1-p\\x_2-q
\end{pmatrix} \delta_p(\de x_1) N(m,\sigma^2)(\de x_2) = f_2g_2\sigma^2
\end{align*}
Note that this is a special case of Example \circled{1}, if we set $m = (p,q)^T$ and $\Sigma = \begin{pmatrix}
0&0\\0&\sigma^2
\end{pmatrix}$.
}
\section{Creating a Hilbert space structure on linear functionals from scratch}\label{sec:rkhs}
We are working in a Banach space, so it is a small miracle that we are able to conjure a Hilbert space structure (but at first only in the realm of functionals $X^*$) out of thin air. Later we will construct an isometry from this Hilbert space into a strict subset of $X$. This Hilbert subspace is the Cameron-Martin space and has a set of very useful properties. But let's start at the beginning:

It is a fundamental fact that every bounded linear functional has second moments with respect to a Gaussian measure, i.e. $X^*\subset L^2(X, \gamma)$. Even more, the embedding
\begin{align*}
j: X^* &\hookrightarrow L^2(X, \gamma)\\
f &\mapsto f - a_\gamma(f)
\end{align*}
is continuous. 
The image of $j$ is not closed in the $L^2$ topology, so we define:
\[ X_\gamma^* = \overline{j(X^*)}^{L^2(X,\gamma)} \text{ is called the reproducing kernel Hilbert space \footnotemark},\]\footnotetext{for reasons explained in remark \ref{rem:RKHS}}
i.e. $X_\gamma^*$ consists of all $L^2$-limits of sequences of the form $(f_n-a_\gamma(f_n))_n$, with $f_n\in X^*$. \ednote{Characterize $X_\gamma^*$ further: All Gaussian random variables?}

This is a definition one easily glances over but there are a few subtleties here that should be pointed out:
\begin{itemize}
\item Although each $f\in X^*$ is also a $f\in L^2(X, \gamma)$ (and by extension, $f-a_\gamma(f)\in X_\gamma^*$), there is quite a difference in viewing $f$ as a linear functional in $X^*$ or a square-integrable random variable: The main point is that two ``directions'' $f_1$, $f_2$ that differ only by a component that is degenerate (i.e. the measure puts no mass along this axis) are \textit{different} objects in $X^*$, but identical as elements in $L^2(X,\gamma)$. Take for example the degenerate Gaussian measure $\delta_0 \otimes N(0,1)$ in two dimensions: Here, $f_1(x) = x_1+x_2$ and $f_2(x) = x_2$ are definitely not the same functional, but as random variables they are identical, because $x_1 = 0$ a.s. This amounts to the fact that $\|f_1-f_2\|_{X^*} = 1$ but $\|f_1-f_2\|_{L^2(X,\gamma)} = 0$.
\item While $f\in X^*$ is a linear functional (i.e. specifically $f(0)=0$), $j(f)$ isn't anymore, but is is rather an affine functional because its mean has been subtracted. On the other hand, $j(f)$ has zero mean, while this is generally not true for $f\in X^*$. This reasoning holds identically for elements $g\in X_\gamma^*$: By definition, $a_\gamma(g) = 0$, but $g(0)\neq 0$ in general. One has to be careful not to mix up those two ideas of ``going through the origin''.
\item By using the covariance inner product $\langle f, g\rangle_{L^2(X, \gamma)} = B_\gamma(f, g) = \int_X [f(x) - a_\gamma(f)]\cdot [g(x)-a_\gamma(g)]\gamma(\de x)$, we obtain a Hilbert space structure in the image of $j$. This will be of great use later. 
\end{itemize}

\myboxtwo{
\subsection*{Example \circled{1}: $X_\gamma^*$ in finite dimensions}
For a multivariate Gaussian $N(m,\Sigma)$ with $m\in \R^d$ and $\Sigma\in \R^{d\times d}$, we have $X = \R^d$ and thus $X^* = \R^d$ in the sense that for $f\in X^*$, the evaluation is scalar multiplication, i.e. $f(x) = f^T x$.

Mean and covariance are $a_\gamma(f) = f^Tm$ and $B_\gamma(f, g) = f^T\Sigma g$ for $f,g\in X^*$. This means that $X_\gamma^*$ consists of objects of the form $j(f) = f - a_\gamma(f)$, hence $j(f) = f(x) - a_\gamma(f) = f^T(x-m)$. In finite dimensions we don't need the $L^2$ completion, every element of $X_\gamma^*$ is actually in this form. 
}

\myboxtwo{
\subsection*{Example \circled{2}: $X_\gamma^*$ for an infinite product of standard Gaussians}
Let $\gamma =  \bigotimes_{j\in\N} N(0,1)$.  As linear functionals are represented by sequences ``eventually zero'', i.e. $f\in X^* = c_{00}$ and the mean $a_\gamma = 0\in\R^\infty$, we don't need to shift any $f$. The $L^2$ closure in the definition of $X_\gamma^*$ is more interesting, though: We first note that $\langle f,g\rangle_{L^2(X, \gamma)} = B_\gamma(f, g) = \sum_{j\in\N}f_j\cdot g_j = \langle f,g\rangle_{l^2}$, so the $L^2(X,\gamma)$ inner product is actually the $l^2$ inner product. As $\overline{c_{00}}^{l^2} = l^2$ (see lemma \ref{lem:c00l2}), the reconstructing kernel Hilbert space $X_\gamma^*$ is the space of square-summable sequences. This proves
\begin{lemma}
$X_\gamma^*$ consists of all functions $f:X\to \R$ of the form
\begin{equation}\label{eq:l2}
f(x) = \sum_{j=1}^\infty a_j x_j
\end{equation}
with $\sum_{j=1}^\infty |a_j|^2 < \infty$. Another way to write this is $X_\gamma^* \cong l^2$.
\end{lemma}
\begin{remark}
Note one subtle point about \eqref{eq:l2}: For an arbitrary $x\in X$, the sum may not converge. It does, however, converge for $\mu$-a.e. $x\in X$. Indeed: Sums of independently and identically distributed (i.i.d.) centered random variables (and the $a_j\cdot x_j$ are i.i.d Gaussians) converge a.s. as soon as the sum of their variances converge. Thus, we only need to show
\[ \sum_{j=1}^\infty a_j^2 \cdot \E x_j^2 < \infty \]
which follows immediately by $\E x_j^2 = 1$ and the condition on $(a_j)_j$.

By the same calculation as in lemma \ref{lem:Gaussian}, we can show that $f$ is a Gaussian random variable with covariance form $B_\gamma(f, g) = \sum_{j=1}^\infty f(e_j)g(e_j)$ where now the sum may contain infinitely many terms (but still is of finite value by the Cauchy-Schwarz inequality in $l^2$).
\end{remark} 


}
\myboxtwo{
\subsection*{Example \circled{3}: $X_\gamma^*$ for a degenerate Gaussian in 2-d}
For, $\gamma = \delta_p \otimes N(q, \sigma^2)$ we established that $X^* = \R^2$. The space $X_\gamma^*$ now consists of linear functionals which are shifted by their mean, i.e. as in Example \circled{1} above if we set $m = (p,q)^T$ and $\Sigma = \operatorname{diag}(0,\sigma^2)$: For any $f\in \R^2$, $j(f) = f_1(x_1-p) + f_2(x_2-q)$ and $X_\gamma^* = j(X^*)\subset L^2(X,\gamma)$ (we don't need the $L^2$ completion in finite dimensions). Note that $j(X^*)$ \emph{looks} 2-dimensional (as we have two parameters, $f_1$ and $f_2$) but this is spurious: Any two $f, g$ that only differ by their  first component, i.e. $f-g = (c, 0)$ have $j(f)$ and $j(g)$ lying in the same $L^2$ equivalence class, i.e. $j(f)-j(g) = j(f-g) = 0\in L^2(X,\gamma)$.
}
By $L^2$ continuity, $a_\gamma$, $\hat \gamma$ and $B_\gamma$ can be extended with their domains changed: For any $X^*$ we can also use elements from $X_\gamma^*$, while for $g\in X_\gamma^*$, $a_\gamma(g) = 0$, $\hat \gamma(f) = \exp\{-\frac{1}{2}\|f\|_{L^2(X,\gamma)}\}$ and $B_\gamma(g_1, g_2) = \int_X g_1(x)g_2(x)\gamma(\de x)$ for $g_1,g_2\in X_\gamma^*$. Note that $X_\gamma^*$ is not a superset of $X^*$ because its elements all need to be centred (as random variables, i.e. $a_\gamma(f) = 0$ for $f\in X_\gamma^*$.

\section{Transporting the Hilbert space structure into the probability space}\label{sec:CM}
So far, we only have a Hilbert space structure in the domain of linear functionals $f\in X^*$ (strictly speaking: on $X_\gamma^*$). We define a functional which will map $X_\gamma^*$ into $X$. This functional will actually be an isometry, so we can transport the Hilbert space structure into a subspace of $X$ (this will be the Cameron-Martin space).

We set
\begin{align*}
R_\gamma : X_\gamma^* & \to (X^*)'\\
R_\gamma f (g) &= \int_X f(x) [g(x)-a_\gamma(g)]\gamma(\de x)
\end{align*}
\begin{remark}
A few notes on the definition of $R_\gamma$:
\begin{itemize}
\item Note that $R_\gamma f \in (X^*)'$, but we want it to be in $X$. This will work along the same lines as the proof that $a_\gamma\in X$ above. We record this in lemma \ref{lem:Rgamma}
\item This functional is very closely related to the covariance inner product $B_\gamma$ (which is the $L^2(X, \gamma)$ inner product). Actually, $R_\gamma f_1 (f_2) = B_\gamma (f_1, f_2)$ for $f_1, f_2\in X_\gamma^*$ (or $R_\gamma f|_{X_\gamma^*} = B_\gamma(f, \cdot)|_{X_\gamma^*}$). The map $R_\gamma$ can be thought of as an asymmetric version of $B_\gamma$ where we straddle ``input'' from $X_\gamma^*$ and $X^*$.
\item $R_\gamma$ needs two inputs (i.e. $f\in X_\gamma^*$ and $g\in X^*$) to be actually explicitly computable, but we will consider the map only with the argument $f$, i.e. as it is stated as a map $X_\gamma^* \to (X^*)'$ (where the parameter $g$ is ``invisible''). We only care about what $R_\gamma$ does with an $f\in X_\gamma^*$.
\item  We don't need to subtract the mean of $f$ (as it is done with $g$) because $f$ has already mean $0$ just by being in $X_\gamma^*$.
\end{itemize}
\end{remark}
\begin{lemma}\label{lem:Rgamma}
$R_\gamma$ can be represented as a map into $X$, i.e. for any $f\in X_\gamma^*$ there is an element $r\in X$ such that for all $g\in X^*$
\[R_\gamma f(g) = g(r)\]
or (for brevity)
\[R_\gamma f(g) = g(R_\gamma f).\]
\end{lemma}
\begin{proof}
This is proven almost identically to lemma \ref{lem:agamma}.
\end{proof}

As $R_\gamma$ can be interpreted as a map $X_\gamma^* \to X$, we can identify $R_\gamma f$ with an element $r_f\in X$ such that 
\begin{equation}\label{eq:reconstructingkernel}
\int_X f(x) [g(x)-a_\gamma(g)]\gamma(\de x) = R_\gamma f(g) = g(r_f),
\end{equation}
but for simplicity we will more sloppily identify $R_\gamma f$ with $r_f$ and write
\[ R_\gamma f(g) = g(R_\gamma f).\]



The most important thing of this section is at its end: We define the Cameron-Martin space\footnote{DaPrato/Zabczyk call $H$ the reconstructing kernel Hilbert space (RKHS), a notation we have reserved for $X_\gamma^*$ (and which remains unnamed in DPZ)} $H$ as the image of $R_\gamma$. We put this in a displayed equation to stress its importance.
\begin{displaymath}
H = \operatorname{ran} R_\gamma
\end{displaymath}
$R_\gamma$ is not onto, so $H\subsetneq X$. There are several senses in which $H$ is ``small'': If $\gamma$ is centred, $\gamma(H) = 0$, i.e. samples of $\gamma$ will almost surely miss $H$; also, the embedding $H\hookrightarrow X$ is compact.

There is an important equivalent characterization for the Cameron-Martin space $H$ which many authors choose as its definition:
\begin{prop}
Define $|h|_H = \sup\{f(h): f\in X^*, \|j(f)\|_{L^2(X, \gamma)} \leq 1\}$, the evaluation norm of $h\in X$. Then the following two statements are equivalent:
\begin{enumerate}
\item $h\in H$, i.e. $h = R_\gamma \hat h$ for some $\hat h\in X_\gamma^*$.
\item $|h|_H < \infty$.
\end{enumerate}
Also, if $h\in H$ (and thus both conditions are true), $|h|_H = \|\hat h\|_{L^2(X,\gamma)}$. This means that $R_\gamma: X_\gamma^* \to H$ is an isometry, rendering $H$ a Hilbert space  with inner product $\langle h, k\rangle_H = \langle \hat h, \hat k\rangle_{L^2(X,\gamma)}$ for $h = R_\gamma \hat h, k = R_\gamma \hat k$.
\end{prop}
\begin{remark}
\begin{itemize}
\item  This means that $H = \{h\in X: |h|_H < \infty\}$ (which is a definition often used in the literature).
\item Note that $\sup\{f(h): f\in X^*: \|f\|_{X^*}\leq 1\} = \|h\|_X$, but the unit ball in $L^2(X,\gamma)$ is bigger in some sense than the unit ball in $X^*$, which means that $|h|_H$ is a stricter norm. In particular: If there are any non-zero $f\in X^*$ with $\|j(f)\|_{L^2(X,\gamma)} = 0$ but $f(h) \neq 0$, (corresponding to degenerate directions of the measure), the $H$-norm is infinite.
\end{itemize}
\end{remark}
\begin{proof}
We show that (1.) $\Rightarrow$ (2.) and assume that $H\ni h = R_\gamma \hat h$ for some $\bar h\in X_\gamma^*$. We want to show that the $H$-norm is finite. Indeed, we recall that $h = R_\gamma \hat h$ is actually the short form of $f(h) = R_\gamma \hat h (f)$ for any $f\in X^*$ and thus
\begin{align*}
f(h) &= R_\gamma \hat h (f) = \int_X \hat h(x) \cdot [f(x)-a_\gamma(f)]\, \gamma(\de x) = \int_X \hat h(x) \cdot j(f)(x)\, \gamma(\de x)\\
&\leq \|\hat h\|_{L^2(X,\gamma)}\cdot \|j(f)\|_{L^2(X,\gamma)},
\end{align*}
which means that 
\begin{align*}
|h|_H &= \sup\{f(h): f\in X^*, \|j(f)\|_{L^2(X, \gamma)} \leq 1 \}\\
&\leq \|\hat h\|_{L^2(X,\gamma)} < \infty.
\end{align*}
For (2.) $\Rightarrow$ (1.) we define for a given $h\in X$ with $|h|_H < \infty$ the evaluation functional
\[ L_h: j(X^*) \to \R,~ j(f) \mapsto f(h). \]
The first issue we need to address is well-definedness: As $j$ is not an one-to-one mapping, there might exist $f_1, f_2\in X^*$ with $j(f_1) = j(f_2)$ and possibly $f_1(h) \neq f_2(h)$. This is impossible, though, as for a given $f\in X^*$
\begin{equation}\label{eq:Lh}
|L_h(j(f))| = |f(h)| \leq |h|_H \cdot \|j(f)\|_{L^2(X,\gamma)}
\end{equation}
and hence for a pair $f_1,f_2\in X^*$ with $j(f_1) = j(f_2)$ (i.e. $j(f_1-f_2) = 0$), we obtain $|f_1(h)-f_2(h)| = |(f_1-f_2)(h)| \leq |h|_H \cdot \|j(f_1-f_2)\|_{L^2(X,\gamma)} = 0$.
Another consequence of \eqref{eq:Lh} is $L^2(X,\gamma)$-continuity and thus we can extend $L_h$ continuously to a (continuous linear) map $L_h: X_\gamma^*\to \R$. As $X_\gamma^*$ is a Hilbert space and $L_h \in (X_\gamma^*)^*$, Riesz' representation theorem shows the existence of an element $\hat h\in X_\gamma^*$ such that 
\begin{equation}\label{eq:reconLhrep}
L_h(\phi) = \langle \hat h, \phi\rangle_{L^2(X,\gamma)} = \int_X \hat h(x)\cdot \phi(x)\,\gamma(\de x)
\end{equation} 
and in particular for any $f\in X^*$ (and thus $j(f)\in X_\gamma^*$):
\begin{equation}\label{eq:reconLh}
f(h) = L_h(j(f)) = \int_X \hat h(x)\cdot j(f)(x)\,\gamma(\de x) = R_\gamma \hat h(f) = f(R_\gamma \hat h) 
\end{equation}

where the last equality is to be understood in the usual duality sense. As the equation $f(h) = f(R_\gamma \hat h)$ is true for any $f\in X^*$, we obtain 
\[ h = R_\gamma\hat h \]
and by definition of $H$, that $h\in H$.

It remains to show that $|h|_H = \|\hat h\|_{L^2(X,\gamma)}$ if any (and thus both) of the equivalent conditions hold. Indeed, for $h = R_\gamma \hat h \in H$, from \eqref{eq:reconLh}
\begin{align*}
\sup\{f(h): f\in X^*, \|j(f)\|_{L^2(X,\gamma)}\leq 1\} &= \sup\left\{\langle \hat h, j(f) \rangle_{L^2(X,\gamma)}:\, f\in X^*, \|j(f)\|_{L^2(X,\gamma)}\leq 1\right\}\\
&= \sup\left\{\langle \hat h, \phi\rangle_{L^2(X,\gamma)}:\, \phi\in X_\gamma^*\right\} = \|\hat h\|_{L^2(X, \gamma)}
\end{align*}
where the step from the first to the second line is valid as $j(X^*)$ is dense in $X_\gamma^*$.
\end{proof}
\begin{remark}\label{rem:RKHS}
Note that \eqref{eq:reconLhrep} identifies the evaluation map $L_h$ with the element $\hat h \in X_\gamma^*$. This is the reason for $X_\gamma^*$ being called ``reconstructing kernel Hilbert space'': The element $\hat h$ is the (integral) kernel which reconstructs the value of $f$ in $h$:
\[ f(h) = \langle \hat h, f\rangle_{L^2(X,\gamma)}.\]
It is truly remarkable that we are able to evaluate $f$ pointwise by alternatively integrate $f$ over the kernel $\hat h$.
\end{remark}
\mybox{\subsection*{Reconstructing kernel Hilbert spaces}
This exposition is borrowed from \cite{paulsen2016introduction}. 
\begin{definition}
A vector subspace of functions $\mathcal H = \{f:D\to \R\}$  on some domain $D$ is called a reconstructing kernel Hilbert space (RKHS) if
\begin{itemize}
\item $\mathcal H$ is a Hilbert space with some inner product $\langle\cdot\rangle$ and
\item For any $y\in D$, the evaluation functional $L_y: \mathcal H \to \R,~ f\mapsto f(y)$ is bounded.
\end{itemize}
\end{definition}
\begin{remark}
Because $\mathcal H$ needs to be a vector space of functions, any Lebesgue space cannot be an RKHS (because $L^p$ functions are actually equivalence classes of functions). This makes sense as pointwise evaluation is not well-defined for those functions.
\end{remark}
As every linear, bounded functional corresponds to an element in the Hilbert space, there is a $k_y\in \mathcal H$ such that $L_y(f) = \langle k_y, f\rangle$. As $k_y(x) = \langle k_x, k_y\rangle = \langle k_y, k_x\rangle = k_x(y)$, this constitutes a  symmetric kernel with a remarkable property:
\begin{definition}
Given an RKHS $\mathcal H$, the symmetric kernel $K:D\times D\to\R,~ K(x,y) = k_y(x)$ is called the \textit{reconstructing kernel}. In this notation, it holds that for any $f\in \mathcal H$,
\[ f(y) = \langle K(\cdot, y), f\rangle. \]
\end{definition}
\paragraph{$H_0^1$ as an RKHS:}
Consider $H_0^1 = \{f\in L^2([0,1]): ~ Df\in L^2([0,1]), f(0)=f(1)=0\}$ where $Df$ is the weak derivative\footnote{and the zero Dirichlet conditions are to be interpreted in the usual sense, i.e. $H_0^1=\overline{C_c^\infty( [0,1])}^{H^1([0,1])}$ }. This is an RKHS with kernel 
\[K(x,y) = \begin{cases}x(1-y) & \text{ for } x\leq y\\(1-x)y & \text{ for } x > y\end{cases}.\]
By standard theory, $H_0^1$ is a Hilbert space with inner product $\langle f, g\rangle = \int_0^1 f'(t)g'(t)\de t$. Evaluation is bounded, as
\begin{align*}
|f(y)| = \left|\int_0^y f'(t) \de t\right| = \left|\int_0^1 f'(t) \chi_{[0,y]}(t)\de t\right| \leq \langle f,f\rangle.
\end{align*}
Thus, $H_0^1$ is an RKHS with the reconstruction property. This is also easily checked:
\begin{align*}
\langle f, K(y, \cdot)\rangle = \int_0^y f'(x) (1-y)\de x + \int_y^1 f'(x) (-y)\de x = f(y).
\end{align*}
\paragraph{The space of band-limited functions as an RKHS}
With\footnote{from \url{https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space}} the Fourier transform $\hat f(\omega) = \int f(x) e^{-i\omega x}\de x$ of some $f\in \{f\in L^2(\R)\}$, the vector space $\mathcal H = : \{f\in L^2(\R):\operatorname{supp}(\hat f)\subset [-a,a]\}$ is an RKHS with kernel $K(x,y) = \frac{\sin(a[y-x])}{\pi[y-x]}$. That $\mathcal H$ with the $L^2$ inner product is an RKHS can be seen from 
\begin{align*}
|f(y)| = \frac{1}{2\pi} \int_{-a}^a \hat f(\omega) e^{i\omega y} \de \omega \leq \sqrt{\frac{a}{\pi}}\cdot \|\hat f\|_{L^2(\R)} = \sqrt{\frac{a}{\pi}}\cdot \|f\|_{L^2(\R)}.
\end{align*}
That $K$ is actually the reconstructing kernel can be intuitively understood as $K$ is the ``bandlimited Dirac'' (i.e. this is the reverse Forier transform of the $[-a,a]$-truncated Fourier transform of the Dirac function $\delta_y(\cdot)$). Indeed $\lim_{a\to\infty} K(x,y) = \delta_y(x)$ (in the sense of distributions), this makes sense as $f(y) = \int f(x) \delta_y(x)\de x$, but the problem here is that the evaluation is not bounded for non-bandlimited functions, so we need $a<\infty$. 

}
\myboxtwo[blue!20]{
\subsection*{Example \circled{1}: $R_\gamma$ in finite dimensions}
For a multivariate Gaussian $N(m, \Sigma)$ with $m\in\R^d$ and $\Sigma \in \R^{d\times d}$ invertible, the map $R_\gamma$ is simply 
\begin{align*}
R_\gamma : X_\gamma^* &\to \R^d\\
(x\mapsto f^T \cdot (x-m)) &\mapsto \Sigma \cdot f.
\end{align*}
In particular,
\[ H = \operatorname{ran} R_\gamma = \R^d \]
as $\Sigma$ is invertible and thus surjective.
For the form of elements in $X_\gamma^*$ read the blue box on this topic in case you don't remember. The inverse $\R_\gamma^{-1}$ is then (in the case where $\Sigma$ is invertible)
\begin{align*}
R_\gamma^{-1}: \R^d &\to X_\gamma^*\\
f &\mapsto (x\mapsto f^T \Sigma^{-1}(x-m)).
\end{align*}
That $R_\gamma$ is actually an isometry can be seen elementarily here: Write $h = R_\gamma \hat h$. Note that $h \in \R^d$ but $\hat h =(x\mapsto f^T(x-m))$ for some $f$, so we can't interpret $\hat h$ as a column vector! Then (note that $j(f) = f - f^Tm,$ i.e. $j(f)(x) = f^T(x-m)$),
\begin{align*}
|h|_H &= \sup\{f(h): f\in \R^d, \|j(f)\|_{L^2(X, \gamma)} \leq 1\} = \sup\{f^Th: f\in \R^d, f^T\Sigma f \leq 1\}\\
&= \sup\{g^T\Sigma^{-1/2}h: |g| \leq 1\} = |\Sigma^{-1/2}h| = \sqrt{h^T\Sigma^{-1}h}
\end{align*}
And also
\begin{align*}
\|\hat h\|_{L^2(\R^d, \gamma)} &= \sqrt{\int_{\R^d}\hat h(x)^2 \gamma(\de x)} = \sqrt{\int_{\R^d}(R_\gamma^{-1} h(x))^2 \gamma(\de x)} = \sqrt{\int_{\R^d}(h^T\Sigma^{-1}(x-m))^2 \gamma(\de x)}\\
&=\sqrt{\int_{\R^d}([\Sigma^{-1}h]^T(x-m))^2 \gamma(\de x)} = \sqrt{[\Sigma^{-1}h]^T\Sigma [\Sigma^{-1}h]} = \sqrt{h^T\Sigma^{-1}h},
\end{align*}
i.e. \[|h|_H = \|\hat h\|_{L^2(\R^d, \gamma)}\]
}

\myboxtwo{
\subsection*{Example \circled{2}: $R_\gamma$ for an infinite product of standard Gaussians}
A calculation shows that $R_\gamma f(g) = \int_X f(x)g(x)\gamma(\de x) = \langle f, g\rangle_{l^2}$, thus we can identify $R_\gamma f = f$ and $R_\gamma$ is the identity. Then the Cameron-Martin space is the image of $R_\gamma$, i.e. $H = R_\gamma X_\gamma^* = X_\gamma^* = l^2$, so the Cameron-Martin space is the space of square-summable sequences. It is even more remarkable in this case that we obtain a Hilbert space structure on $H$, which is a subspace of $X$, because $X$ is not even a normed space! Here, $|h|_H = \|h\|_{l^2} = \|h\|_{L^2(X,\gamma)}$ (in this case $R_\gamma$ is the identity, so $h = \hat h$).
}

\myboxtwo{
\subsection*{Example \circled{3}: $R_\gamma$ for a degenerate Gaussian in 2-d}
The calculations are similar to Example \circled{1}, but we need to take care as $\Sigma$ is not invertible. $R_\gamma$ is the same:
\begin{align*}
R_\gamma: X_\gamma^* &\to \R^2\\
(x\mapsto f^T(x-(p,q)^T))&\mapsto (0, \sigma^2 f_2)^T
\end{align*}
The image of $R_\gamma$ (the Cameron-Martin space) is $\{0\}\times \R = H$ and the inverse of $R_\gamma$ is
\begin{align*}
R_\gamma^{1}: H &\to X_\gamma^*\\
\begin{pmatrix}
0\\f_2
\end{pmatrix} & \mapsto  \frac{f_1(x_1-p) + f_2(x_2-q)}{\sigma^2}
\end{align*}
where $f_1$ is arbitrary because any value $f_1\in\R$ will yield the same element in $X_\gamma^*$ because they will have the same value $\gamma$-almost-surely.
}

\section{Shifts of Gaussian measures}
The main point of the Cameron-Martin theory is dealing with shifts of Gaussian measures, i.e. given a Gaussian measure $\gamma$ in some Banach space, we define $\gamma_h$ by setting $\gamma_h(B) = \gamma(B-h)$ for sets $B\in \mathcal B(X)$. Our main goal is to obtain the density of $\gamma_h$ with respect to $\gamma$, i.e. find the function $\rho_h: X\to \R$ such that 
\[\gamma_h(B) = \int_B \rho_h(x) \gamma(\de x),\]
or symbolically, $\frac{\de \gamma_h}{\de \gamma}(x) = \rho_h(x)$.
 There are two difficulties with this ``relative density'': The first problem arises when we try to shift in directions in which the measure $\gamma$ is degenerate. In this case, we transport our probability mass to an area which had zero probability mass, and thus formally $\frac{\de \gamma_h}{\de \gamma}(x) = \mlq\mlq\frac{C}{0}\mrq\mrq$ and we cannot expect to get a density. 
 The second issue only arises in infinite dimension and is most intuitively explained with looking at the measure $\otimes_{n=1}^J N(0,1)$ with $J\to \infty$.
 We start with our non-degenerate example in finite dimensions as this is illustrative for the general theory:
\myboxtwo{
\subsection*{Example \circled{1}: Gaussian Shifts in finite dimensions}
We take a non-degenerate Gaussian in $d$ dimensions, i.e. $\gamma = N(m, \Sigma)$ with $\Sigma\in \R^{d\times d}$ non-singular. Define $\gamma_h$ as $\gamma_h(\cdot) = \gamma(\cdot - h)$ and then (by elementary multivariate Gaussian wizardry or the help of a Computer algebra system)
\begin{displaymath}
\frac{\de \gamma_h}{\de \gamma}(x) = \exp\left\{h^T\Sigma^{-1}(x-m) - \frac{1}{2}h^T\Sigma^{-1}h\right\} = \exp\left\{(R_\gamma^{-1}(h))(x) - \frac{1}{2}|h|_H^2\right\},
\end{displaymath}
It turns out that this form is valid even in infinite dimensions, whenever $|h|_H$ is finite.
} 

The following lemma is useful:
\begin{lemma}[{\cite[Lemma 3.1.4]{Lunardi}}]\label{lem:prelim_CM}
Take a Gaussian measure $\gamma$ and a $g\in X_\gamma^*$. Recall that the characteristic function of $\gamma$ is $\hat \gamma(f) = \exp(ia_\gamma(f) - \|j(f)\|_{L^2(X, \gamma)}^2/2)$, signifying that $a_\gamma$ is the mean and $B_\gamma(f, g) = \int_X j(f)(x)j(g)(x)\gamma(\de x)$ is the covariance form. Then the measure given by
\[\frac{\de\mu_g}{\de \gamma}(x) = \exp\left\{g(x)-\frac{1}{2}\|g\|_{L^2(X,\gamma)}\right\}\]
is a Gaussian probability measure with characteristic function
\[\hat\mu_g(f) = \exp\left\{i[a_\gamma(f) + R_\gamma g(f)]-\frac{1}{2}\|j(f)\|_{L^2(X,\gamma)}^2\right\},\]
i.e. is the Gaussian measure with the same covariance structure as $\gamma$ but with mean shifted by $R_\gamma g$. We can think of the mean as the object $a_\gamma + R_\gamma g$.
\end{lemma}
\begin{proof}[Sketch of proof]
We show that the characteristic function of $\mu_g$ has the form as stated. We do the following formal calculation for $f\in X^*$:
\begin{align*}
\hat \mu_g(f) &= \int_X e^{if(x)}\mu_g(\de x) = \int_X e^{if(x)+g(x)} \mu_g(\de x) \cdot e^{-\frac{1}{2}\|g\|_{L^2(X,\gamma)}^2} = e^{-\frac{1}{2}\|g\|_{L^2(X,\gamma)}^2} \cdot \int_X e^{i[f(x)-ig(x)]}\gamma(\de x) \\
&= e^{-\frac{1}{2}\|g\|_{L^2(X,\gamma)}^2} \cdot \hat \gamma(f-ig) = \exp\left({-\frac{1}{2}\|g\|_{L^2(X,\gamma)}^2}\right) \cdot \exp \left(i a_\gamma(f-ig)-\frac{1}{2}B_\gamma(f-ig,f-ig)\right)\\
\intertext{Now note that $a_\gamma(g) = 0$, $B_\gamma(f,g) = R_\gamma g(f)$, $B_\gamma(g,g) = \|g\|_{L^2(X,\gamma)}^2$ and $B_\gamma(f,f) = \|j(f)\|_{L^2(X,\gamma)}^2$, hence}
&= \exp\left({-\frac{1}{2}\|g\|_{L^2(X,\gamma)}^2}\right) \cdot \exp \left(i a_\gamma(f)-\frac{1}{2}\|j(f)\|_{L^2(X,\gamma)}^2 + iR_\gamma g(f) + \frac{1}{2}\|g\|_{L^2(X,\gamma)}^2\right)\\
&= \exp\left\{i[a_\gamma(f) + R_\gamma g(f)]-\frac{1}{2}\|j(f)\|_{L^2(X,\gamma)}^2\right\}
\end{align*}
Note that this calculation is not actually valid without further justification as in the step from first to second line we evaluated the characteristic function $\hat \gamma$ in the linear continuous functional $f-ig$ which is not real (But we defined the functional only for real linear continuous functionals). This can be done rigorously by a few more lines of arguments from complex analysis, as done in the proof in \cite{Lunardi}, but the formal calculation yields enough insight for our purpose. 
\end{proof}
\begin{remark}
\begin{itemize}
\item Lemma \ref{lem:prelim_CM} is \textit{almost} what we want: We obtain a density of a shifted Gaussian with respect to its unshifted reference measure, but we cannot choose the shift directly: For a given $g\in X_\gamma^*$ we obtain a shift of $R_\gamma g$ in the mean. We need to reverse this in order to choose any shift we want. 
\item We can already see here why not every shift will be feasible: With lemma \ref{lem:prelim_CM} we can only acquire shifts which are in the range of $R_\gamma$ -- this is exactly the Cameron-Martin space. This means that we can only take shifts $h\in H$ from the Cameron-Martin space if we want to get a measure which -- shifted by $h$ -- are absolutely continuous w.r.t. their unshifted reference measure (i.e. we have a density). The Cameron-Martin theorem collects all these statements.
\end{itemize} 
\end{remark}
\begin{theorem}[{\cite[Theorem 3.1.5]{Lunardi}}]\label{thm:CM}
Let $\gamma$ be a Gaussian measure on a Banach space $X$. For $h\in X$, define the shifted measure $\gamma_h$ by $\gamma_h(\cdot) = \gamma(\cdot - h)$.
\begin{itemize}
\item If $h\in H$, then the measure $\gamma_h$ is equivalent to $\gamma$ and $\frac{\de \gamma_h}{\de \gamma }(x) = \rho_h(x)$ with
\[\rho_h(x) = \exp\left\{R_\gamma^{-1} h(x) - \frac{1}{2}|h|_H^2\right\}.\]
\item If $h\not\in H$, then $\gamma_h \bot \gamma$ (the measures are singular with respect to each other).
\end{itemize}
This means that two Gaussian measures differing only by a shift in the domain are either equivalent (if the shift is in the Cameron-Martin space) or singular. There is no middle ground as in the case of the Lebesgue measure and the Lebesgue measure on $[0,1]$, where the latter is absolutely continuous w.r.t. the other but not the other way around.
\end{theorem}
\begin{remark}
When comparing lemma \ref{lem:prelim_CM} and \ref{thm:CM}, $g = R_\gamma^{-1} h$ and equivalently $R_\gamma g = h$, i.e. we see that we can only realize shifts $h$ which are in the range of $R_\gamma$, i.e. the Cameron-Martin space. Note that $|h|_H = \|R_\gamma^{-1} h\|_{L^2(X,\gamma)} = \|g\|_{L^2(X,\gamma)}$. The proof relies on lemma \ref{lem:prelim_CM} for the first statement. To show singularity in the case $h\not\in H$, it uses the notion of Hellinger distance (which is a distance between measures and is not covered here) to show that $\gamma$ and $\gamma_h$ are ``too far away from each other'' to have any joint support.
\end{remark}
\myboxtwo{\subsection*{Example \circled{1}: Gaussian Shifts in finite dimensions (cont'd)}
We already saw that the formula we derived by elementary calculation matches the formula for Theorem \ref{thm:CM} in the ``equivalence'' case. In our case, $\Sigma$ is invertible, and thus $H = \R^d$, which makes sense: If we shift a non-degenerate multivariate Gaussian, we will obtain an equivalent measure. So we can't obtain a singular measure by shifting. The next two examples show two reasons why equivalence might \emph{not} work out.
}
\myboxtwo{\subsection*{Example \circled{2}: Gaussian Shifts for an infinite product of standard Gaussians}
We derived before that $H = l^2$, i.e. we can only shift the mean by an element of $l^2$. In particular, for $h = (1,1,\ldots)$, $\gamma = \bigotimes N(0, 1)$ and $\gamma_h = \bigotimes N(1, 1)$ are singular with respect to each other (although the shift is not ``that big''! For a given feasible shift $h\in l^2$, i.e. $\gamma_h = \bigotimes_{j\in\N} N(h_j, 1)$, the Cameron-Martin theorem yields
\[ \frac{\de \gamma_h}{\de \gamma}(x) = \exp \left\{\langle h, x\rangle_{l^2} - \frac{1}{2}\|h\|_{l^2}^2\right\}.\]
Another way to see where the problem comes in when having infinite dimensions is the following: Let's change our measure to $\gamma = \bigotimes_{j=1}^J N(0,1)$, i.e. we don't have infinitely many Gaussians but only ``very many''. Then basic intuition (and our finite-dimensional considerations) tell us that for any shift $h\in\R^J$, the measures $\gamma$ and $\gamma_h$ should be equivalent. This is indeed the case and the Cameron-Martin formula reduces to
\[ \frac{\de \gamma_h}{\de \gamma}(x) = \exp \left\{\sum_{j=1}^Jh_jx_j - \frac{1}{2}\sum_{k=1}^J h_k^2\right\}.\]
This is a well-defined term for any fixed $J$ (and thus the density exists), but in the limit $J\to\infty$, the sum $\sum_{k=1}^J h_k^2$ becomes the $l^2$-norm of $h$ and we have to be careful which shift $h$ we choose.
}
\myboxtwo{\subsection*{Example \circled{3}: Gaussian shifts for a degenerate Gaussian in 2-d}
We know that $H = \{0\}\times \R$, so we can only shift in the second component. This makes sense as the complete probability mass of $\gamma$ rests on the axis $x_1=p$, so we can't move it in $x_1$-direction without losing equivalence. The Cameron-Martin formula states that for $h = (h_1,0)^T \in H$ (recall that $R_\gamma^{-1} (0,h_2)^T = (h_1(x_1-p)+h_2(x_2-q))/\sigma^2$ for arbitrary $h_1\in\R$, 
\[ \frac{\de \gamma_h}{\de \gamma}(x) = \exp\left\{\frac{h_1(x_1-p)}{\sigma^2}\right\}\cdot \exp\left\{\frac{h_2(x_2-q)}{\sigma^2} - \frac{h_2^2}{2\sigma^2} \right\} \]
Is the multplicative factor with arbitrary $h_1$ a contradiction? No: For $x_1 = p$ this factor is the identity and on $x_1 \neq p$ the measure $\gamma$ is not supported. We can try to put as much value on the density of $\gamma_h$ w.r.t. $\gamma$, but by courtesy of $C\cdot 0 = 0$ we will obtain the same result.

For simplicity it makes sense to choose $h_1=0$ and thus we write
\[ \frac{\de \gamma_h}{\de \gamma}(x) = \exp\left\{\frac{h_2(x_2-q)}{\sigma^2} - \frac{h_2^2}{2\sigma^2} \right\} \]
}
\section{Interesting infinite-dimensional examples}\label{sec:InfDimEx}
So far, the infinite product of i.i.d Gaussians was our only infinite-dimensional example. In this section, we will look at some Banach-space-valued examples you are more likely to meet in the wild.
\myboxtwo{
\subsection*{Example \circled{4}: Gaussian measures in Hilbert space}
The Hilbert space case is \textit{almost} identical by naively going with dimension to infinity in the finite-dimensional case of example \circled{1} but there is a striking difference in the form of the RKHS and the Cameron-Martin space.

We record a few facts about operators in Hilbert spaces which we will need. Consult \cite{Lunardi} and \cite{brezis2010functional} for proofs and details. Let $X$ be an infinite-dimensional separable Hilbert space with norm $\|\cdot\|$ and inner product $\langle\cdot,\cdot\rangle$. We identify $X^*$ with $X$ via Riesz' representation theorem. We call $\mathcal L(X)$ the set of linear and bounded operators $X\to X$.
\begin{itemize}
\item $L\in \mathcal L(X)$ is nonnegative iff $\langle Lx,x\rangle \geq 0$ for all $x\in X$.
\item $L\in \mathcal L(X)$ is called compact iff $L$ is the limit of a sequence of finite rank operators in the operator norm.
\item $L\in \mathcal L(X)$ is called self-adjoint iff $\langle Lx, y\rangle = \langle x, Ly\rangle$ for any $x,y\in X$.
\item A nonnegative, adjoint operator is called \textit{nuclear} or \textit{of trace-class} iff there is an orthonormal basis $\{e_k: k\in\N\}$ of $X$ such that 
\[\operatorname{trace}(L) := \sum_{k=1}^\infty \langle Le_k,e_k\rangle < \infty.\]
\item Every nuclear operator is compact.
\item Every operator $L\in\mathcal L(X)$ that can be written in the form
\[Lx = \sum_{k=1}^\infty \alpha_k \langle x, f_k\rangle f_k \]
for some orthonormal basis $\{f_k:k\in\N\}$ of $X$ is compact.
\item For every compact and self-adjoint operator $L\in\mathcal L(X)$ there exists an orthonormal basis of $X$ given by $\{e_k:k\in\N\}$ consisting of eigenvectors of $L$ such that 
\begin{equation}\label{eq:formGaussianHilbert} Lx = \sum_{k=1}^\infty \lambda_k \langle x, e_k\rangle,
\end{equation}
where $Le_k = \lambda e_k$ for $\lambda_k\in\R$. If $L$ is non-negative, all $\lambda_k$ are non-negative.
\end{itemize}
\paragraph{On the form of $a_\gamma$ and $B_\gamma$ in the Hilbert space setting:} Due to Riesz' representation theorem, we have a more direct way of dealing with $a_\gamma$ and $B_\gamma$: Take a Gaussian measure in a Hilbert space (but regard it as a Banach space). Then continuity of $a_\gamma : X^*\to \R$ and $B_\gamma: X^*\times X^*\to \R$ suffices (and we don't need weak-*-continuity in this case) that there is an element $a\in X$ such that $a_\gamma(f) = \langle f, a\rangle$ and there is a self-adjoint operator $Q\in\mathcal L(X)$ such that $B_\gamma(f,g) = \langle Q f, g\rangle$, hence
\begin{displaymath}
\langle Qf, g\rangle = \int_X \langle f, x-a\rangle \langle g, x-y\rangle \, \gamma(\de x).
\end{displaymath} 
We call $Q$ the covariance operator. The measure's characteristic function is
\begin{equation}\label{eq:charHilbert}
\hat \gamma(f) = \exp\left\{i\langle f, a\rangle - \frac{1}{2}\langle Qf, f\rangle\right\}
\end{equation}


The next theorem states exactly which form Gaussian measures in Hilbert spaces can have.

\begin{theorem}
\begin{enumerate}
\item If $\gamma$ is a Gaussian measure on $X$, then its characteristic function is given by \eqref{eq:charHilbert} for some $a\in X$ and a self-adjoint, non-negative trace-class operator $Q\in \mathcal L(X)$.
\item For any $a\in X$ and a self-adjoint, non-negative trace-class operator $Q\in \mathcal L(X)$, the characteristic function in \eqref{eq:charHilbert} constitutes a Gaussian measure on $X$.
\end{enumerate}
\end{theorem}

\begin{remark}
This theorem and \eqref{eq:formGaussianHilbert} shows that Gaussian measures $\gamma$ in infinite-dimensional Hilbert spaces are actually very simple: Given the measure's mean $a$ and covariance operator $Q$, we have a basis of orthonormal eigenfunctions, such that $Qe_k = \lambda_k e_k$, with $\lambda_k \geq 0$, over which the measure diagonalizes. Then $a = \sum_{k=1}^\infty a_k\cdot e_k$ and we can say that
\begin{displaymath}
\gamma = \bigotimes_{k=1}^\infty N(a_k, \lambda_k),
\end{displaymath}
i.e. a product of infinitely many independent Gaussians with mean $a_k$ and variance\footnote{be careful with squares here: usually we say that $\sigma^2$ is a variance, i.e. $\sqrt{\lambda_k}$ is the standard deviation here} $\lambda_k$. The correspondence here is that the $k$-th factor on the right hand side belongs to direction $e_k$ and equivalently: A Gaussian random variable $x\in X$ is spanned as follows:
\begin{equation}
x = \sum_{k=1}^\infty (a_k + \eta_k\cdot \sqrt{\lambda_k})\cdot e_k,\quad \eta_k \sim N(0,1) \text{ i.i.d.}
\end{equation}
\end{remark}
\paragraph{On RKHS and Cameron-Martin space in the Hilbert space setting: }
Now we are ready to understand what the Cameron-Martin space of a Hilbert-space-valued Gaussian measure looks like: Linear continuous functionals $f\in X^* = X$ are just elements of the Hilbert space with $f(x) = \langle f, x\rangle$ for any $x\in X$, and mean and covariance are given by $a\in X$ and $Q\in \mathcal L(X)$ as argued above. Then $j(f) = f-a$. The next theorem collects all interesting objects regarding Cameron-Martin theory in the context of Gaussians on Hilbert spaces.
\begin{theorem}[Theorem 4.2.7 in \cite{Lunardi}]\label{thm:Hilbert}
Let $\gamma = N(a,Q)$ be a non-degenerate Gaussian measure on a Hilbert space $X$. Recall that there is a orthonormal basis of eigenfunctions of $Q$, i.e. $Qe_k = \lambda_k e_k$.
\begin{enumerate}
\item The space of continuous linear functionals is $X^* = X$, i.e. linear continuous functionals are of the form $X^* = \{g:X\to\R:~ g(x) = \langle z,x\rangle = \sum_{k=1}^\infty \langle z, e_k\rangle \langle x, e_k\rangle \text{ for some $z\in X$}\}$.
\item The RKHS is 
\[X_\gamma^* = \left\{f: X\to \R:~ f(x) = \sum_{k=1}^\infty \lambda_k^{-1/2} \langle x-a, e_k\rangle \langle z, e_k\rangle = \langle Q^{-1/2}z,x-a\rangle \text{ for some $z\in X$}\right\}.\]
\item The isometry map is $R_\gamma: \left(x\mapsto \sum_{k=1}^\infty \lambda_k^{-1/2} \langle x-a, e_k\rangle \langle z, e_k\rangle\right) \mapsto Q^{1/2} z$.
\item The Cameron-Martin space is 
\[H = R_\gamma X_\gamma^* = Q^{1/2}(X) = \left\{x\in X:~ \sum_{k=1}^\infty x_k^2 \lambda_k^{-1}<\infty\right\}.\]
\item For $h = Q^{1/2}z$, we have $h = R_\gamma \hat h$ with $\hat h(x) = \sum_{k=1}^\infty \langle x-a, e_k\rangle \langle z, e_k\rangle$ and $\langle h, k\rangle_H = \langle Q^{-1/2} h, Q^{-1/2}k\rangle$.
\end{enumerate}
\end{theorem}
The proof can be found in \cite{Lunardi} but we want to point out one thing here.
\begin{remark}
The Hilbert space case is in many ways like a infinite-dimensional analogue of our example \circled{1}, i.e. the matrix-case in finite dimensions. There is one striking difference in the definition of $X_\gamma^*$ and $R_\gamma$, though: While in finite dimensions, an element of $X_\gamma^*$ is given by $f: X\to \R, x\mapsto \langle z, x-a\rangle_{\R^d}$ for some $z\in X$ (originally we identified $z$ and $f$, also we have switched to inner product notation here), the infinite-dimensional analogue is $f: X\to \R, x\mapsto \sum_{k=1}^\infty \langle x-a, e_k\rangle\langle Q^{-1/2}z, e_k\rangle = \langle Q^{-1/2}z, x-a\rangle$, i.e. there is a factor of $Q^{-1/2}$ unaccounted for. In finite dimensions we can easily insert and remove any number of $Q$s (in the non-degenerate case), as $\left\{f:\R^d\to\R: x\mapsto \langle Q^{17}\tilde z, x-a\rangle\text{ for some $\tilde z\in \R^d$ }\right\}$ is the same set as the one in the original definition of $X_\gamma^*$ in $\R^d$ and hence we choose the easiest one (which drops the $Q$). In infinite dimensions we can't do without it as the following example shows:

Take $w^{(n)} = e_n$, i.e. the $n$-th basis element. Set $a=0$ for simplicity. Define a functional $f_n: x\mapsto \langle x, w^{(n)}\rangle = \langle x, e_n\rangle$. Then a quick reflection shows that $\|f_n\|_{L^2(X,\gamma)} = \lambda_n \xrightarrow{n\to\infty} 0$ and hence $f_n\to 0 = f$ in $L^2(X,\gamma)$. The problem is that $w^{(n)}$ does not converge (to $0$ or anything else) in $X$ and thus we \textit{cannot} write $f: x\mapsto \langle x, \lim_{n\to\infty} w^{(n)}\rangle$ (although this would be desirable, even only from an aesthetics point of view). The trick here is to factor out one order of magnitude $Q^{-1/2}$ which stops $w^{(n)}$ from converging:
 Define $z^{(n)} = Q^{1/2} w^{(n)} = \lambda_n^{1/2} e_n$, then    the functional $f_n$ can be re-written as $f_n: x\mapsto \langle Q^{-1/2} z^{(n)}, x\rangle$ (compare this with the definition of $X_\gamma^*$ in theorem \ref{thm:Hilbert}). In this case still $f_n\to 0$ (we haven't changed $f_n$) but also $\|z_n\|^2 = \lambda_n \to 0$, i.e. convergence of $f_n$ goes hand in hand with convergence of $z_n$. By the general idea of everything going on with Cameron-Martin theory, this is desirable: $f_n$ is a sequence in the RKHS, $z^{(n)}$ is the corresponding isomorphic sequence in Cameron-Martin space. The sequence $w^{(n)}$ is just weird and meaningless. With this idea in mind, everything else works as well. I encourage you to read the whole proof in \cite{Lunardi}.
\end{remark}
\paragraph{Shifts of Gaussians in the Hilbert space setting:} From theorem \ref{thm:Hilbert} we see immediately that we can shift a Gaussian exactly by vectors in the range of $Q^{1/2}$ in order to obtain an equivalent Gaussian measure.}

\myboxtwo{
\subsection*{Example \circled{5}: Gaussian random variables on $l^1$}
Set $X = l^1$ and $\gamma = \bigotimes_{k=1}^\infty \mathcal N(0, \sigma_k^2)$. In order to make sure (using the Kolmogorov two-series theorem) that samples from this measure are in $l^1$ a.s., we need to enforce
\begin{displaymath}
\sum_k \sigma_k < \infty,\quad \sum_k \sigma_k^2 < \infty.
\end{displaymath}
As is common, we identify $(l^1)^* = l^\infty$ in the sense that for $f\in (l^1)^*$, $f(x) = \sum_k f_k\cdot x_k$ for the sequence $(f_k)_k \in l^\infty$. 
Then, $a_\gamma = 0\in X^*$ and 
\begin{align*}
B_\gamma(f,g) &= \int_X \left(\sum_k f_k x_k\right) \cdot \left(\sum_l g_l x_l\right) \gamma(\mathrm d x)\\
&= \sum_{k} f_k g_k \int_{\mathbb R} x_k^2 \,\mathcal N(0, \sigma_k^2)(\mathrm d x_k)\\
&= \sum_k f_k g_k \sigma_k^2
\end{align*}
The reconstructing Kernel Hilbert space is computed in the following lemma:
\begin{lemma}
\begin{align*}
X_\gamma^* := \overline{j(l^\infty)}^{L^2(X,\gamma)} &= \left\{f: l^1\to \mathbb R: ~ f(x) = \sum_{k=1}^\infty x_k\cdot \frac{z_k}{\sigma_k},~ z\in l^2\right\}\\
&= \left\{\left(\frac{z_k}{\sigma_k}\right)_k: (z_k)_k \in l^2\right\}\\
&= \left\{(y_k)_k: \sum_{k=1}^\infty y_k^2\sigma_k^2 < \infty\right\}\end{align*}
\end{lemma}
\begin{proof}
The equivalence of all representations on the right hand side is easily seen so we just need to show the equality 
\[ \overline{j(l^\infty)}^{L^2(X,\gamma)} = V:= \left\{f: l^1\to \mathbb R: ~ f(x) = \sum_{k=1}^\infty x_k\cdot \frac{z_k}{\sigma_k},~ z\in l^2\right\}.\]
\paragraph{``$\supseteq$'':} Consider an element $f\in V$, hence there is a $z\in l^2$ such that
\[f(x) = \sum_{k=1}^\infty x_k\cdot \frac{z_k}{\sigma_k}.\]
We need to show that $f \in \overline{j(l^\infty)}^{L^2(X,\gamma)}$, i.e. that it is an $L^2$-limit of elements in $j(l^\infty)$. Note that $j$ is the plain inclusion $l^\infty \hookrightarrow L^2(X,\gamma)$ because the measure $\gamma$ is centered. We can define explicitly
\[(f^{(n)})_n \subset l^\infty, \quad f^{(n)}(x) = \sum_{k=1}^n x_k \cdot \frac{z_k}{\sigma_k}.\]
We will show that this sequence of elements in $j(l^\infty)$ (this inclusion is trivial because of the finite amount of terms in each $f^{(n)}$) converges in $L^2(X,\gamma)$ to $f$, which is exactly what we need. Let without loss of generality $m<n$, then
\begin{align*}
\|f^{(n)}-f^{(m)}\|_{L^2(X,\gamma)}^2 &= \sum_{k=m+1}^n \frac{z_k^2}{\sigma_k^2} \cdot \int_{\mathbb R} x_k^2 \,\mathcal N(0, \sigma_k^2)(\mathrm d x_k) \\
&= \sum_{k=m+1}^n z_k^2 
\end{align*}
which converges to $0$ for $m,n\to \infty$ because $z\in l^2$. Hence, $f^{(n)}$ is a Cauchy sequence in $L^2(X,\gamma)$. As this space is complete, the limit $\tilde f = \lim_n f^{(n)}$  exists and $\tilde f = f$.
\paragraph{``$\subseteq$'':} Take $f\in X_\gamma^*$, then by definition there exists a sequence $g^{(n)}$ of elements in $l^\infty$ such that $g^{(n)} \to f$ in $L^2(X, \gamma)$. We show that $f$ can be represented by an element of $V$. First, we recall that $g^{(n)} = (g^{(n)}_k)_k$ and we define $z_k^{(n)} := \sigma_k \cdot g^{(n)}_k$. By convergence of the $g^{(n)}$, we also have convergence of the $g^{(n)}_k$, i.e. we can define
\[z_k := \lim_{n\to\infty} z_k^{(n)} = \sigma_k \cdot \lim_{n\to\infty}  g^{(n)}_k.\]
 Then 
\[g^{(n)}(x) = \sum_{k=1}^\infty g^{(n)}_k\cdot x_k = \sum_{k=1}^\infty \frac{z_k^{(n)}}{\sigma_k}\cdot x_k.\]
We claim that the sequence $(g^{(n)})_n$ converges (again in $L^2$) to $\tilde f$, where 
\[\tilde f(x) := \sum_{k=1}^\infty x_k\cdot \frac{z_k}{\sigma_k},\]
where the $z_k$ have been defined above. This is indeed the case, as
\begin{align*} \|f - \tilde f\|_{L^2(X,\gamma)}^2 &\leq \|f - g^{(n)}\|_{L^2(X,\gamma)}^2 + \|g^{(n)} - \tilde f\|_{L^2(X,\gamma)}^2 \\
&\leq \|f - g^{(n)}\|_{L^2(X,\gamma)}^2 + \sum_{k=1}^\infty \frac{|z_k^{(n)} - z_k|^2}{\sigma_k^2}\cdot \sigma_k^2 \\
&\leq \|f - g^{(n)}\|_{L^2(X,\gamma)}^2 + \sum_{k=1}^N |z_k^{(n)} - z_k|^2 + \sum_{k=N+1}^\infty \underbrace{\frac{|z_k^{(n)} - z_k|^2}{\sigma_k^2}}_{\leq C, \text{ as } g, g^{(n)}\in l^\infty}\cdot \sigma_k^2\\
&\leq \|f - g^{(n)}\|_{L^2(X,\gamma)}^2 + \sum_{k=1}^N |z_k^{(n)} - z_k|^2 + C\cdot  \sum_{k=N+1}^\infty \sigma_k^2 \end{align*}
Now we set $N$ large enough such that $\sum_{k=N+1}^\infty \sigma_k^2 < \frac{\epsilon}{3C}$, further $n$ large enough such that $|z_k^{(n)} - z_k|^2 < \frac{\epsilon}{3N}$ for all $k=1,\ldots,N$ and $n$ (also) large enough such that $\|f-g^{(n)}\|_{L^2(X,\gamma)} < \epsilon/3$. Then, all in all, $\|f-\tilde f\|_{L^2(X,\gamma)}$, is arbitrarily small and hence equal in $L^2(X,\gamma)$. As $\tilde f\in V$, we have shown $X_\gamma^*\subseteq V$.
\end{proof}
The Cameron--Martin space is now $H = R_\gamma X_\gamma^*$, see the following lemma.

\begin{lemma}
The mapping $R_\gamma : X_\gamma^*\to X$ is $R_\gamma ((w_k)_k) = (\sigma_k^2 w_k)_k$ and the Cameron--Martin space is \[H = \{(y_k)_k:~ \|y\|_H^2 := \sum_{k\in\mathbb N} \frac{y_k^2}{\sigma_k^2} < \infty\}.\]
\end{lemma}
\begin{proof}
Take $f\in X_\gamma^*$ (i.e. $f = (z_k/\sigma_k)_k$ for some $z\in l^2$) and $g\in X^*$, then by definition,
\begin{align*}
R_\gamma f(g) &= \int_X f(x) (g(x)-a_\gamma(g))\gamma(\mathrm d x) = \sum_{k=1}^\infty \frac{z_k}{\sigma_k}\cdot g_k \cdot \sigma_k^2 = \sum_{k=1}^\infty z_k\cdot g_k \cdot \sigma_k \\
&= g((\sigma_k\cdot z_k)_k),
\end{align*}
where in the last step we used $g\in X^* = l^\infty$. Then by the duality theory outlined above, we can identify
\[R_\gamma f = R_\gamma\left( \left(\frac{z_k}{\sigma_k}\right)_k\right) = (\sigma_k\cdot z_k)_k,\]
which is the first part of the lemma's statement. 
The Cameron--Martin space is
\begin{align*}
H &:= R_\gamma X_\gamma^* = R_\gamma\left\{\left(\frac{z_k}{\sigma_k}\right)_k:~ z\in l^2\right\}\\
&= \{(\sigma_k\cdot z_k)_k: z\in l^2\}\\
&=\left\{(y_k)_k:~ \sum_{k\in\mathbb N} \frac{y_k^2}{\sigma_k^2} < \infty\right\}.
\end{align*}
Note finally that $H\subset l^1$, as for any $y\in H$,
\begin{align*}
\|y\|_1 = \sum_{k=1}^\infty |y_k| = \sum_{k=1}^\infty \frac{|y_k|}{\sigma_k}\cdot \sigma_k \leq \sqrt{\sum_{k=1}^\infty \frac{|y_k|^2}{\sigma_k^2}}\cdot \sqrt{\sum_{k=1}^\infty {\sigma_k^2}} \leq \|y\|_H\cdot \sqrt{\sum_{k=1}^\infty {\sigma_k^2}}.
\end{align*}
\end{proof}
}
\subsection{Gaussian processes}
We consider the special case $X = \{f \in C([0,1]):~ f(0) = 0\}$, i.e. we want to construct Gaussian measures on the space of continuous functions over the unit interval with zero initial condition. One choice leads to Brownian motion (the corresponding measure is called the \textit{classical Wiener space}), another to the Ornstein-Uhlenbeck process etc. We start with a few facts on Gaussian processes.

\begin{itemize}
\item A one-dimensional stochastic process $x_t, t\in[0,1]$ is called a \textit{Gaussian process}, if for any $n\in \N$ and  $t_1,\ldots, t_n$, the random vector $(x_{t_1},\ldots,x_{t_n})$ has a joint Gaussian distribution on $\R^n$.
\item If the process is continuous, this gives a measure on $X  =C([0,1])$. According to Nate Eldredge: ``If there is any good in the world, this ought to be an example of a Gaussian measure [on $X$]''. 
\item The set of linear continuous functionals $X^*$ is the set of finite signed Borel measures on $[0,1]$. Note that by taking Dirac measures $\delta_{t_1},\ldots,\delta_{t_n}$, this constitutes exactly the $n$-dimensional distributions of the stochastic process. The set of all finite combinations of Diracs is weak-*-dense in $X^*$. Note that $X^*$ contains also measures\footnote{Extreme care has to be taken here not to confuse the Gaussian measure on $C([0,1])$ established by the Gaussian process and linear continuous functionals, which are measures on $[0,1]$.} like $\lambda_{[0,1]}$, i.e. with notation $x_\cdot\in X$, we obtain a 1-dimensional random variable $\nu(x_\cdot) = \int_0^1 x_t \de t$. 
\item Any centred Gaussian process is completely determined by its covariance function $C(s,t) = \E x_s x_t$.
\end{itemize}
\paragraph{On the form of $a_\gamma$ and $B_\gamma$ in the Gaussian processes setting: }
\begin{lemma}
A centred one-dimensional Gaussian process $x_t, t\in[0,1]$ with covariance function $C(\cdot,\cdot)$ constitutes a Gaussian measure $\gamma$ on $C([0,1])$ with mean $a_\gamma = 0\in X = C([0,1])$ and covariance $B_\gamma(\nu_1,\nu_2) = \int_0^1\int_0^1 C(s,t)\nu_1(\de s)\nu_2(\de t)$.
\end{lemma}
\begin{proof}
That $\gamma$ is actually a Gaussian measure follows from a short consideration about the weak-*-density of Diracs in $X^*$. This means that any $\nu\in X^*$ is a pointwise limit of Diracs. Also, evaluation of $\nu $ on a specific $x\in X$ leads to the finite-dimensional distributions $(X_{t_1},\ldots,X_{t_n})$ which are patently Gaussian. This means that any $\nu\in X^*$ is a pointwise limit of linear continuous functionals $\nu_n$ with $\nu_n(x)$ Gaussian and thus $\nu(x)$ is also Gaussian. This shows that $\gamma$ is Gaussian. The details can be found in section 4.4 of Nate Eldredges lecture notes \cite{eldredge2016analysis}.

Now we only need to show the form of the covariance form. By definition,
\begin{align*}
B_\gamma(\nu_1, \nu_2) &= \int_X \nu_1(x) \nu_2(x)\gamma(\de x) = \int_X \left(\int_0^1 x_t \nu_1(\de t)\right)\cdot \left(\int_0^1 x_s\nu_2(\de s)\right) \gamma(\de x)
\intertext{Note that $x\in C([0,1])$ is a complete Gaussian process where $x_t$ is its value at position $t\in [0,1]$. Then Fubini's theorem (justified by boundedness of all terms involved which is a result of Fernique's theorem) allows us to swap the order of integrations, yielding}
&= \int_0^1\int_0^1 \int_X x_t x_s \gamma(\de x) \nu_1(\de t)\nu_2(\de s) = \int_0^1\int_0^1 \E x_t x_s \nu_1(\de t)\nu_2(\de s)\\
&=  \int_0^1\int_0^1 C(s, t) \nu_1(\de t)\nu_2(\de s)
\end{align*}
Let's appreciate for a minute the beaty of this expression: The covariance of two linear functionals is calculated by a kernel-type integral with the (spatial) covariance function as its kernel integral.
\end{proof}
\paragraph{On RKHS and Cameron-Martin space in the Gaussian processes setting: }
The isometry map $R_\gamma$ is calculated as follows: Because all objects are centred, we don't care about the mean and hence
\begin{align*}
R_\gamma \nu_1(\nu_2) = B_\gamma(\nu_1, \nu_2) = \int_0^1\int_0^1 C(s,t) \nu_1(\de s) \nu_2(\de t) = \nu_2\left(\int_0^1 C_(s,\cdot)\nu_1(\de s)\right),
\end{align*}
thus $R_\gamma \nu_1 \in C([0,1])$ with $(R_\gamma\nu_1) (t) =\int_0^1 C(s,t)\nu_1(\de s) $. The image of $R_\gamma$, i.e. the Cameron-Martin space, can then be written down explicitly for a concrete covariance function $C$.
\myboxtwo{\subsection*{The classical Wiener space (Brownian motion)}
Brownian motion is the Gaussian process with covariance function $C(s,t) = \min(s,t)$. In this case, $B_\gamma(\nu_1,\nu_2) = \int_0^1 \int_0^1 \min(s,t)\nu_1(\de s)\nu_2(\de t)$. 
\begin{lemma}
The Cameron-Martin space of the Gaussian measure generated by Brownian motion on $X=C([0,1])$ is $H = \{h\in C([0,1]): \text{ the weak derivative $Dh$ exists and } Dh\in L^2([0,1])\}$. The inner product is given by $\langle h_1,h_2\rangle_H = \int_0^1 Dh_1(t)Dh_2(t)\de t$.
\end{lemma}
\begin{proof}
We show that the mapping $R_\gamma$ is an isometry from $X_\gamma^*$ to the proposed space (which can then be isometrically identified with the Cameron-Martin space). To this end, we call $\tilde H$ the proposition for the Cameron-Martin space defined in the statement of this lemma. Because working with actual elements of $X_\gamma^*$ is hard, we only work with the (weak-*-dense) subset of finite superposition of Diracs. Note that $(R_\gamma \delta_s)(t) = \int_0^1 \min(r, t)\delta_s(\de r) = \min(s, t)$ and thus $D(R_\gamma \delta_s)(t) = \chi_{[0, s]}(t)$. 
\begin{align*}
\langle R_\gamma \delta_s, R_\gamma \delta_t\rangle_{\tilde H} = \int  D(R_\gamma\delta_s)(r) D(R_\gamma\delta_t)(r)\de r = \chi_{[0, s]}(r)\chi_{[0, t]}(r) \de r = \min(s, t) = C(s,t) = B_\gamma(\delta_s, \delta_t)
\end{align*}
As this constitutes an isometric isomorphism from the space of finite superpositions of Diracs (a weak-*-dense subset of $X_\gamma^*$) to $\tilde H$, by extension to $X_\gamma^*$ we obtain an isometric isomorphism from $X_\gamma^*$ to $\tilde H$, i.e. $\tilde H = H$ up to isomorphism.
\end{proof}
The take-away message of this is that we can shift any path of Brownian motion by a path which is in $H$ and still obtain an equivalent measure.
}
\section{The Feldman-Hajek theorem}
This section is mostly interesting for readers who want to study Bayesian inverse problems. \ednote{Missing}
\appendix
\section{\Frechet spaces}
In order to fully understand the $\R^\infty$ example \circled{2}, we record a few facts about \Frechet spaces here. Let $X$ be a topological vector space with topology $\T$. A few preliminary definitions:
\begin{definition}
A set $C$ is called
\begin{itemize}
\item balanced if for all $x\in C$ and all $|\lambda| \leq 1$ also $\lambda x \in C$.
\item convex if for all $x, y\in C$ and all $0\leq t \leq 1$, also $tx+(1-t)y\in C$.
\item absorbent if $\bigcup_{t>0} tC = X$.
\item absolutely convex, if $C$ is both convex and balanced. This is the same as saying that for every $x,y\in C$ and real numbers $|t|+|s|=1$, $tx+sy \in C$. The best image for this is that $C$ contains every parallelogramm with vertices $x$ and $y$ symmetric to the origin.
\end{itemize} 
\end{definition}
\begin{definition}
\begin{itemize}
\item A collection of sets $\mathcal B \subset \T$ is called a base of the topology $\T$ if for every open set $U\in\T$ there is a $B\in \mathcal B$ with $B\in U$.
\item A collection of sets $\mathcal B_x \subset \T$ is called a local base at $x$ of the topology $\T$ if for every neighborhood $U\in\T$ of $x$ (i.e. $x\in U$) there is a $B\in \mathcal{ B}_x$ with $B\in U$.
\end{itemize}
\end{definition}

We are going to compare two definitions of locally convex spaces.

\begin{minipage}[t]{0.45\textwidth}
\begin{definition}\label{def:l1}
A topological vector space $X$ is called \textit{locally convex} if the origin has a local base of absolutely convex absorbent sets.
\end{definition}
\end{minipage}
\hfill
\begin{minipage}[t]{0.45\textwidth}
\begin{definition}\label{def:l2}
A topological vector space $X$ is called \textit{locally convex} if there is a family of seminorms generating the topology.
\end{definition}
\end{minipage}

\begin{lemma}
Definitions \ref{def:l1} and \ref{def:l2} are equivalent.
\end{lemma}
\begin{proof}
{\textit{Proof of \ref{def:l1} $\Rightarrow$ \ref{def:l2}.}}
Define the Minkowski gauge of a set $C\in X$ as 
\[\mu_C(x) = \inf\{\lambda> 0: x\in \lambda C\}.\]
Then we can show that the origin's local base of absolutely convex absorbent sets $\mathcal{B}_0 = \{B_a, a\in I\}$ constitute a family of seminorms via their Minkowski gauges $\{\mu_{B_a}, a\in I\}$ and the latter generates the topology of $X$. XXX

{\textit{Proof of \ref{def:l2} $\Rightarrow$ \ref{def:l1}}}
Using the family of seminorms we can easily build a local base of absolutely convex absorbent sets for the origin.XXX
\end{proof}

\begin{remark}\label{rem:locconv}

\begin{enumerate}
\item In Definition \ref{def:l2}, note that the family of seminorms is not necessarily countable.
\item If the family of seminorms $\|\cdot\|_k$ generating the topology is \textit{countable}, then this constitutes a pseudometric generating the topology. To see this, define
\begin{equation} d(x,y) = \sum_{k=1}^\infty 2^{-k} \frac{\|x-y\|_k}{1+\|x-y\|_k}.\label{eq:metric}
\end{equation}
Note that this pseudo	metric is neither unique (so the metrization is not unique) nor homogenous (so we can't define a suitable norm). 
\item As translation is continuous in any t.v.s, we can use a base for the origin as a base for any point by translation.
\end{enumerate}
\end{remark}

\begin{lemma}\label{lem:hausdorff}
Let $X$ be a locally convex TVS with its family of seminorms $(\| \cdot \|_\alpha)_{\alpha\in I}$. Then $X$ is Hausdorff if and only if
\begin{equation}\label{eq:hausdorff}
\forall \alpha\in I:~ \|x\|_\alpha = 0\quad \Leftrightarrow \quad x = 0
\end{equation}
\end{lemma}
\begin{proof}
Let $X$ be Hausdorff. We show \eqref{eq:hausdorff}. If for every $\alpha\in I$, $\|x\|_{\alpha}=0$ and we assume that $x\neq 0$, we know by the Hausdorff property that there exists an open neighborhood $U_x$ of $x$ and another open neighborhood of $0$ such that they do not intersect. As the topology is generated by the seminorms, we can set $U_x = \{u:~ \|u-x\|_{\alpha} < \eps~ \forall k\leq K\}$ for some $K\in \N, \eps>0$. Now obviously $0\in U_x$ as $\|x\|_{\alpha} = 0$ for all $\alpha$. Hence there cannot exist any neighborhood of $0$ such that we can separate $x$ and $0$. The other direction of \eqref{eq:hausdorff} is obvious by the definition of seminorms.

Now suppose \eqref{eq:hausdorff} holds. If we set $x\neq y$, by \eqref{eq:hausdorff} we know that there is at least one $\alpha\in I$ such that $\|x-y\|_\alpha = M \neq 0$.

By setting $U_x =\{u\in X:~\|u-x\|_\alpha < M/2 \}$ and $V_y =\{v\in X:~\|v-y\|_\alpha < M/2 \}$, we have $U_x\cap V_y = \emptyset$ by the triangle inequality (assume there is a $w$ both in $U_x$ and $V_y$, then one can show that $\|x-y\|_\alpha \leq \|x-w\|_\alpha + \|w-y\|_\alpha < M$).
\end{proof}

We are going to compare two definitions of Fr\'echet spaces.\newline

\begin{minipage}[t]{0.45\textwidth}
\begin{definition}\label{def:f1}
A \Frechet space $X$ is a topological vector space $X$ such that
\begin{enumerate}
\item $X$ is locally convex, i.e. $0\in X$ has a local base of absorbent and absolutely convex sets.
\item The topology $\T$ can be induced by a translation invariant metric $d$, i.e. $d(x+a, y+a) = d(x, y)$.
\item $(X, d)$ is a complete metric space.
\end{enumerate}
\end{definition}
\end{minipage}
\hfill
\begin{minipage}[t]{0.45\textwidth}
\begin{definition}\label{def:f2}
A \Frechet space $X$ is a topological vector space $X$ such that
\begin{enumerate}
\item The topology can be induced by a countable family of seminorms $\|\cdot \|_k$, i.e. $U$ open if and only if for all $u\in U$ there is an integer $K \geq 0$ and an $\eps > 0$ such that \[\{v: \|v-u\|_k<\eps:~ k\leq K\} \subset U\]
\item $X$ is Hausdorff.
\item $X$ is complete w.r.t. the family of seminorms $\|\cdot\|_k$, i.e. if $(x_m)_m$ is a Cauchy sequence w.r.t. all seminorms, then there exists an $x\in X$ with $x_n\to x$ w.r.t. $\|\cdot \|_k$ (and by property 2 even $x_n\to x$).
\end{enumerate}
\end{definition}
\end{minipage}

\begin{remark}
\begin{enumerate}
\item Note that the items 1-3 are not all ``parallely equivalent", but as a set, they are equivalent:
\begin{itemize}
\item \ref{def:f1}.1 and \ref{def:f1}.2 $\Rightarrow$ \ref{def:f2}.1 (the countability property of the family of seminorms is stronger than plain local convexity)
\item \ref{def:f1}.2 $\Rightarrow$ \ref{def:f2}.2 (metrizability implies Hausdorff)
\item \ref{def:f2}.1 and \ref{def:f2}.2 $\Rightarrow$ \ref{def:f1}.2
\item \ref{def:f2}.1$\Rightarrow$ \ref{def:f1}.1 
\item \ref{def:f1}.3 $\Leftrightarrow$ \ref{def:f2}.3
\end{itemize}
\end{enumerate}
\end{remark}
\begin{lemma}
Definitions \ref{def:f1} and \ref{def:f2} are equivalent.
\end{lemma}
\begin{proof}
\textbf{\ref{def:f1} $\Rightarrow$ \ref{def:f2}.}

ad 1.): From the equivalent definitions of locally convex vector spaces we know that \ref{def:f2}.1 is \textit{almost} local convexity. The trouble is that we claim the existence of a \textit{countable} family of seminorms whereas local convexity just gives some family of seminorms. By combining \ref{def:f1}.1 (local convexity in its set-theoretic version) and \ref{def:f1}.2 (metrizability), we obtain \ref{def:f2}.1.

Indeed, the metric $d$ allows us to define small balls $B_n = \{x:~ d(x,0)< \frac{1}{n}\}$ for each $n\in\N$. As the topology can be induced by $d$, we know that there is an open set inside each $B_n$ and by local convexity we know of the existence of an absorbent and absolutely convex set $C_n \subset B_n$. Those sets in turn define via their Minkowski gauges $\mu_n$ a countable family of seminorms. Equivalence of the topology of $X$ and the topology generated by the family $\mu_n$ follows from the fact that the $C_n$ are a local base of the topology of $X$.

ad 2.): Every metric (or metrizable) space is Hausdorff.

ad 3.): This is a direct consequence of completeness of $d$.

\textbf{\ref{def:f2} $\Rightarrow$ \ref{def:f1}}
ad 1.): As noted above, \ref{def:f2}.1 is slightly stronger than \ref{def:f1}.1.

ad 2.): By \ref{def:f2}.2 we know by lemma \ref{lem:hausdorff} that the countable family of seminorms in \ref{def:f2}.1 fulfills
\[\forall n\in \N:~ \|x\|_k = 0 \quad \Leftrightarrow \quad x = 0.  \] 
This property makes the pseudometric in \eqref{eq:metric} (which is one possible pseudometrization) a proper metric. The fact that the topology of $X$ is induced by $d$ is equivalent to \ref{def:f2}.1.
\end{proof}
\section{The space $\R^\infty$}
\begin{lemma}\label{lem:RInfTop}
The topological vector space $X = \R^\infty = \{(x_n)_{n\in\N}: ~x_n\in\R:~\forall n\in\N \}$ with the product topology (i.e. the topology generated by all sets of the form $U_1\times\cdots\times U_m\times \R^\infty$ with all $U_i\subset \R $ open) is a \Frechet space. One possible metrization is
\begin{equation}
d(x, y) = \sum_{n=1}^{\infty}2^{-n}\cdot \frac{|x_n-y_n|}{1 + |x_n-y_n|}
\end{equation}
\end{lemma}
\begin{proof}
We will prove this by showing that $\R^\infty$ fulfills every item in definition \ref{def:f2}.

ad 1.): We define $\|x\|_k := |x_k|$, i.e. the absolute value of the $k$-th item. This is obviously a seminorm on $\R^\infty$. We show that the product topology an be generated by this family of seminorms. 

Let $U'$ be an open set in the product topology. Then $U'$ is a union of sets of the form $U_1\times\cdots\times U_m\times \R^\infty$. It suffices to consider only open sets out of this basis. Let $U = U_1\times\cdots\times U_m\times \R^\infty$. Choose a point $u\in U$, i.e. $u = (u_1, \ldots, u_m, u_{m+1},\ldots)$ where $u_i \in U_i$ for $i=1,\ldots, m$. Define $\delta = \min_{i=1}^M \dist(u_i, \del U_i)$, where $\dist$ is the distance function on $\R$. Then the set $\{v: \|v-u\|_k < \delta: k\leq M\} $ is a subset of $U$, which constitutes the first part of Definition \ref{def:f2}.1. The other direction is obvious by definition of the product topology.

ad 2.): By lemma \ref{lem:hausdorff} we know that we only need to show that if and only if for given $u\in\R^\infty$ and all $n\in \N$ we have $\|u\|_k = |u_k| = 0$, then $u=0\in\R^\infty$. But this is obvious.

ad 3.): This is easily shown by using completeness of $\R$ in each dimension of $\R^\infty$.
\end{proof}

\begin{lemma}
The space $\R^\infty$ with the product topology is a polish space.
\end{lemma}
\begin{proof}
$\R^\infty$ is separable as $\Q^\infty$ is countable and dense and it is completely metrizable by definition of \Frechet spaces.
\end{proof}

\begin{lemma}
The product topology of $\R^\infty$ cannot be generated by a norm. 
\end{lemma}
\begin{proof}
Open sets of $\R^\infty$ are necessarily unbounded but balls defined by any norm must be (by definition) bounded.
\end{proof}
\begin{lemma}
The Borel $\sigma$-algebra is the same as the product $\sigma$-algebra.
\end{lemma}
\begin{proof}
The Borel-$\sigma$-algebra is the $\sigma$-algebra generated by open sets (i.e. sets of the form $U_1\times\cdots\times U_m\times \R^\infty$), which is the same as the product $sigma$-algebra.
\end{proof}
We write $e_i$ for the element of $X$ with $e_i(j) = \delta_{ij}$. Any element $x\in X$ can be written as $x = \sum_{i=1}^\infty x_i e_i$.


\section{Some results about sequences}
\begin{lemma}[$(\R^\infty, \|\cdot\|_\infty)$ is complete]
Let $(a^{(k)})_{k\in\N}$ be a Cauchy sequence (of sequences) in the space of sequences with the supremum norm, i.e.
\[\text{for all } \eps > 0 \text{ there is a } K_\eps \text{ s. t. for all } k,l\geq K_\eps: \|a^{(k)} - a^{(l)}\|_\infty = \sup_n |a_n^{(k)} - a_n^{(l)}| < \eps.\footnote{\text{We can think of }$(a^{(k)})_k$ \text{ being a ``uniformly Cauchy'' sequence.}}  \]
Then $a^{(k)} \to a$ in $(\R^\infty, \|\cdot\|_\infty)$, i.e.  there is a sequence $a\in\R^\infty$ such that
\[\text{for all } \eps > 0 \text{ there is a } M_\eps \text{ s. t. for all } k\geq M_\eps: \|a^{(k)} - a\|_\infty = \sup_n |a_n^{(k)} - a_n| < \eps, \]
i.e. $a^{(k)}$ converges uniformly to $a$. \label{lem:Cauchy}
\end{lemma}
\begin{proof}
We use completeness of $\R$ when we realize that by assumption for every $\eps > 0$ there is an index $N_\eps$ such that for every $n\in\N$ there is a limit $a_n$ such that for all $l \geq N_{\eps}$, 
\[ |a_n^{(l)} - a_n| < \eps. \]
Now we fix $\eps > 0$ and we want to bound $|a_n^{(k)}-a_n| < \eps$ for all $n\in\N$ by choosing $k$ large enough. For this, we write
\begin{align*}
|a_n^{(k)}-a_n| &\leq |a_n^{(k)}-a_n^{(l)}| + |a_n^{(l)} - a_n|
\end{align*}
Now first we choose $K = K_{\eps/2}$, $N = N_{\eps/2}$ and $l \geq \max\{K, N\}$. This makes the second part smaller than $\eps/2$. As soon as we choose $k\geq K$, the first part is also smaller than $\eps/2$. Hence we can choose $M_\eps = K_{\eps/2}$ and we are done.
\end{proof}

\begin{lemma}[$\overline{c_{00}}^{l^2} = l^2$]\label{lem:c00l2}
Define $c_{00} = \{(a_n)_{n\in\N}: \exists m:~ \forall n\geq m:~ a_n = 0\}$ and choose a Cauchy sequence in $c_{00}$ w.r.t. the $l^2$-norm, i.e.
\[\text{for all } \eps > 0 \text{ there is a } N_\eps \text{ s. t. for all } k,l\geq N_\eps: \sum_{n=1}^\infty |a_n^{(k)} - a_n^{(l)}|^2 < \eps.\]
Then \[a^{(k)} \to a ~ \text{as a sequence in $l^2$}\]
\end{lemma}
\begin{proof}
We need to show that there is a sequence $a$ with
\begin{enumerate}
\item $\|a^{(k)} - a\|_2 \xrightarrow{k\to\infty} 0$ and
\item $a \in l^2.$
\end{enumerate}
The existence of such an $a$ follows immediately by dropping the summation symbol and using completeness of $\R$ to obtain a limit element $a_n$ for every $n\in \N$, which constitutes a sequence $a = (a_n)_{n\in\N}$.

Now for 1.) we see that for any $R\in\N$
\begin{align*}
\sum_{n=1}^R |a_n^{(k)}-a_n|^2 &\leq  \sum_{n=1}^R 2\cdot |a_n^{(k)}-a_n^{(l)}|^2 + 2\cdot |a_n^{(l)}-a_n|^2.
\end{align*}
If we choose $l \geq M := M_{\sqrt\frac{\eps}{4\cdot R}}$ (the index from Lemma \ref{lem:Cauchy}), the second sum is bounded by $\eps/2$. After setting $N := N_{\sqrt{\frac{\eps}{4}}}$ and claiming additionally $k,l\geq N$ (thus $l\geq\max\{M,N\}$), we see that uniformly in $R$ we just need to set $k \geq N$ in order to bound 
\[\sum_{n=1}^R |a_n^{(k)}-a_n|^2  < \eps \]
and thus the bound also holds for the infinite sum, which proves 1.).

For 2.) we see that 
\begin{align*}
\sum_{n=1}^R |a_n|^2 &\leq \sum_{n=1}^R 2 \cdot|a_n-a_n^{(k)}|^2 + 2\cdot | a_n^{(k)}|^2
\end{align*}
which is finitely bounded uniformly in $R$ as we can set $k$ such that the first sum is arbitrarily small and the second sum is some finite value (the $l^2$-norm of $a^{(k)}$.
\end{proof}
\section{Acknowledgments}
I'd like to thank especially Nate Eldredge, whose excellent lecture notes have helped me a lot. This document is basically a re-interpretation of the first 20 pages of \cite{eldredge2016analysis} by filling in the ``exercises'' posted there and removing some of the more technical lemmata. If you have worked through this document and want to learn more, I strongly encourage you to give Nate's notes a look. The ISEM lecture notes \cite{Lunardi} set a slightly different focus and tend to fully work out the details in the proofs.

My motivation for writing this stems from the article ``Research Debt'' on \texttt{distill.pub}\footnote{\url{http://distill.pub/2017/research-debt/}}: It has cost me a lot of time to get an intuition for what properties and pitfalls infinite-dimensional Gaussians have and maybe a few people can save themselves the ``hard climb'' by profiting from my particular way of understanding them.
\bibliographystyle{siam}
\bibliography{lit}
\end{document}
